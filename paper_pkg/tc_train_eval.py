#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
paper_pkg.tc_train_eval (CLEAN)

Core training/evaluation module for Transformer + Consistency (TC), with realtime guardrails.

This is the import-safe version:
- NO CLI main()
- Imports env/execution from paper_pkg.env_core
- Used by paper_pkg.train and paper_pkg.eval
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from types import SimpleNamespace
from typing import List, Tuple, Dict, Any
import copy
import json
import time
import math

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# === ê¸°ì¡´ ì‹¤í—˜ íŒŒì¼ì—ì„œ ì¬ì‚¬ìš© ===
from .env_core import (
    ExperimentConfig,
    build_env,
    execute_plan_on_env,
    sustainable_dag_plan,
    summarize_runs,
)

# === ëª¨ë¸ import (í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ë§ì„ ìˆ˜ ìˆìŒ) ===
try:
    from models.proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator
except ImportError:
    from proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator


# =========================================================
# Config
# =========================================================

@dataclass
class TrainEvalRealtimeConfig:
    exp_cfg: ExperimentConfig = field(default_factory=ExperimentConfig)

    # ë°ì´í„° ë¶„í•  (ì˜ˆì‹œ)
    train_seeds: List[int] = field(default_factory=lambda: [0])
    val_seeds: List[int] = field(default_factory=lambda: [1])
    test_seeds: List[int] = field(default_factory=lambda: [2])

    # ëª¨ë¸/í•™ìŠµ
    L: int = 20
    H: int = 30
    epochs: int = 3
    stride: int = 5
    lr: float = 1e-3
    lambda_consistency: float = 1.0
    reward_invalid_penalty: float = -1.0

    # ---- í†µí•© ë³´ìƒ(mini) í•˜ì´í¼íŒŒë¼ë¯¸í„° ----
    # (ë‹¨ì¼ í•­ê³µê¸° ì‹œë‚˜ë¦¬ì˜¤ì´ë¯€ë¡œ LBëŠ” ì œì™¸)
    reward_gamma: float = 0.97                 # í• ì¸ê³„ìˆ˜ Î³
    reward_w_utility: float = 1.00             # ê°€ìš© ìŠ¬ë¡¯ utility ë³´ìƒ
    reward_w_outage: float = 0.70              # ë¹„ê°€ìš©(outage) íŒ¨ë„í‹°
    reward_w_switch: float = 0.05              # í•¸ë“œì˜¤ë²„(ìŠ¤ìœ„ì¹˜) íŒ¨ë„í‹°
    reward_w_robustness: float = 0.20          # í›„ë³´ ë…¸ë“œì˜ ë¯¸ë˜ ê°€ìš©ì„±(robustness) ë³´ë„ˆìŠ¤
    reward_w_jitter: float = 0.15              # ì§§ì€ ìœ ì§€ì‹œê°„ ê¸°ë°˜ ì§€í„° proxy íŒ¨ë„í‹°
    reward_w_pingpong: float = 0.20            # A->B->A í˜•íƒœ ping-pong proxy íŒ¨ë„í‹°
    reward_jitter_min_hold_ratio: float = 0.35 # ì´ ë¹„ìœ¨ë³´ë‹¤ ì§§ê²Œ ìœ ì§€ë˜ë©´ ì§€í„° íŒ¨ë„í‹° ê°•í™”
    reward_use_discounted_return: bool = True  # í• ì¸ ëˆ„ì ë³´ìƒ ì‚¬ìš© ì—¬ë¶€
    reward_guided_target_scale: float = 1.0    # RL-loss ê°€ì¤‘ ìŠ¤ì¼€ì¼ (ë³´ìƒ ê¸°ë°˜)
    reward_guided_target_min_weight: float = 0.1
    reward_guided_target_max_weight: float = 3.0

    # í‰ê°€ ì‹œ consistency sampling steps
    consistency_steps_eval: int = 2

    # ì‹¤ì‹œê°„ ë³´ì • (online correction)
    rt_reliability_alpha: float = 0.90  # EMA: í´ìˆ˜ë¡ ëŠë¦¬ê²Œ ë³€í•¨
    rt_low_reliability_threshold: float = 0.45
    rt_fp_extra_penalty: float = 0.90    # false positive(ì˜ˆì¸¡=1 ì‹¤ì œ=0) ì¶”ê°€ ê°ì‡ 
    rt_use_actual_current_slot_fallback: bool = True


    # ---- ì‹¤ì‹œê°„ HO ì•ˆì •í™” ê°€ë“œë ˆì¼(í•‘í/ê³¼ë„í•œ HO ì–µì œ) ----
    # enable_guardrails=Falseë¡œ ë‘ë©´ ê¸°ì¡´ ë™ì‘(ë§¤ ìŠ¬ë¡¯ ì¦‰ì‹œ ë³´ì •/ìŠ¤ìœ„ì¹˜)ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
    rt_enable_guardrails: bool = True

    # (1) ìµœì†Œ ìœ ì§€ì‹œê°„: ìµœê·¼ ìŠ¤ìœ„ì¹˜ í›„ rt_min_dwell ìŠ¬ë¡¯ ë™ì•ˆì€ ìŠ¤ìœ„ì¹˜ ê¸ˆì§€(í˜„ì¬ ì—°ê²°ì´ ìœ íš¨í•œ ê²½ìš°)
    rt_min_dwell: int = 3

    # (2) íˆìŠ¤í…Œë¦¬ì‹œìŠ¤: ìŠ¤ìœ„ì¹˜í•˜ë ¤ë©´ í˜„ì¬ ëŒ€ë¹„ ì¶©ë¶„í•œ ì´ë“ì´ ìˆì–´ì•¼ í•¨
    # - ratio ê¸°ì¤€: score_new >= score_cur * (1 + rt_hysteresis_ratio)
    # - abs ê¸°ì¤€: (ì„ íƒ) score_new - score_cur >= rt_hysteresis_abs
    rt_hysteresis_ratio: float = 0.08
    rt_hysteresis_abs: float = 0.0

    # (3) í•‘í(A->B->A) ì–µì œ: ìµœê·¼ rt_pingpong_window ìŠ¬ë¡¯ ë‚´ ë˜ëŒë¦¼ì´ë©´ ë” í° íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ìš”êµ¬
    rt_pingpong_window: int = 4
    rt_pingpong_extra_hysteresis_ratio: float = 0.12

    # ---- Realtime planning enhancements ----
    rt_enable_pending: bool = True
    rt_fallback_mode: str = "lookahead"  # myopic | lookahead
    rt_fallback_alpha_latency: float = 0.05
    rt_fallback_gamma: float = 0.97
    rt_fallback_H: int = 30
    rt_debug_log: bool = False

    # ì €ì¥
    results_dir: str = "results"
    weight_path: str = "results/trained_weights_rl_real_env.pth"
    split_manifest_path: str = "results/train_val_test_seed_split.json"


# =========================================================
# Utilities
# =========================================================

def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def update_ema_target(online_net, target_net, tau=0.05):
    with torch.no_grad():
        for online_param, target_param in zip(online_net.parameters(), target_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)


def save_weights(transformer, consistency, weight_path: str):
    Path(weight_path).parent.mkdir(parents=True, exist_ok=True)
    torch.save(
        {
            "transformer_state_dict": transformer.state_dict(),
            "consistency_state_dict": consistency.state_dict(),
        },
        weight_path,
    )
    print(f"âœ… weights saved: {weight_path}")


def load_weights(transformer, consistency, weight_path: str, device: torch.device):
    ckpt = torch.load(weight_path, map_location=device)
    transformer.load_state_dict(ckpt["transformer_state_dict"])
    consistency.load_state_dict(ckpt["consistency_state_dict"])


# =========================================================
# Seed collection (actual / predicted env pair)
# =========================================================

def collect_env_pairs(cfg: ExperimentConfig, seeds: List[int]) -> List[Dict[str, Any]]:
    pairs = []
    for seed in seeds:
        print(f"[collect] seed={seed}")
        actual_env = build_env(cfg, seed=seed, predicted_view=False)
        pred_env = build_env(cfg, seed=seed, predicted_view=True)

        # ê¸°ë³¸ shape ì²´í¬
        if actual_env.A_final.shape != pred_env.A_final.shape:
            raise ValueError(f"shape mismatch at seed={seed}: actual {actual_env.A_final.shape} vs pred {pred_env.A_final.shape}")
        if actual_env.A_final.shape[1] != pred_env.A_final.shape[1]:
            raise ValueError(f"N mismatch at seed={seed}")

        pairs.append(
            {
                "seed": seed,
                "actual_env": actual_env,
                "pred_env": pred_env,
            }
        )
    return pairs



# =========================================================
# Reward helpers (mini integrated reward for single-aircraft)
# =========================================================

def _best_available_node(A_row: np.ndarray, U_row: np.ndarray) -> int:
    avail = np.where(A_row == 1)[0]
    if len(avail) == 0:
        return -1
    return int(avail[np.argmax(U_row[avail])])


def _contiguous_available_run_len(future_A_true: np.ndarray, start_i: int, node_idx: int) -> int:
    """start_ië¶€í„° ì—°ì† ê°€ìš©(1)ì¸ ê¸¸ì´. ì²« ìŠ¬ë¡¯ì´ 0ì´ë©´ 0."""
    H, _ = future_A_true.shape
    run_len = 0
    for tau in range(int(start_i), H):
        if int(future_A_true[tau, node_idx]) != 1:
            break
        run_len += 1
    return int(run_len)


def _discounted_integrated_reward_single_aircraft(
    future_A_true: np.ndarray,   # (H, N)
    future_U_true: np.ndarray,   # (H, N)
    target_time_offset: int,
    target_node_idx: int,
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> float:
    """
    ë…¼ë¬¸ì‹ í†µí•© ë³´ìƒì˜ 'ë¯¸ë‹ˆ ë²„ì „' (ë‹¨ì¼ í•­ê³µê¸°):
      - utility í•­ (ì •ê·œí™”ëœ utility ì‚¬ìš©)
      - outage íŒ¨ë„í‹°
      - switch(í•¸ë“œì˜¤ë²„) ë¹„ìš©
      - robustness ë³´ë„ˆìŠ¤(ì„ íƒ ë…¸ë“œì˜ ë¯¸ë˜ ê°€ìš© ë¹„ìœ¨)
      - jitter proxy íŒ¨ë„í‹° (ì§§ì€ ìœ ì§€ì‹œê°„ì— ìŠ¤ìœ„ì¹˜í• ìˆ˜ë¡ ë¶ˆë¦¬)
      - ping-pong proxy íŒ¨ë„í‹° (A->B->A)
    â€» Load Balance(LB)ëŠ” ë‹¨ì¼ í•­ê³µê¸°ë¼ ì œì™¸
    """
    H, N = future_A_true.shape
    i = int(np.clip(target_time_offset, 0, H - 1))
    n = int(np.clip(target_node_idx, 0, N - 1))

    gamma = float(cfg_te.reward_gamma)
    w_u = float(cfg_te.reward_w_utility)
    w_o = float(cfg_te.reward_w_outage)
    w_s = float(cfg_te.reward_w_switch)
    w_r = float(cfg_te.reward_w_robustness)
    w_j = float(getattr(cfg_te, "reward_w_jitter", 0.0))
    w_p = float(getattr(cfg_te, "reward_w_pingpong", 0.0))
    jitter_thr = float(getattr(cfg_te, "reward_jitter_min_hold_ratio", 0.35))

    if i >= H:
        return float(cfg_te.reward_invalid_penalty)

    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    total = 0.0

    # ì„ íƒ ì‹œì ì— 1íšŒ ìŠ¤ìœ„ì¹˜ ë¹„ìš©
    switch_happens = bool(current_node != -1 and current_node != n)
    if switch_happens:
        total -= w_s

        # ---- Jitter proxy: ìŠ¤ìœ„ì¹˜í–ˆëŠ”ë° ê³§ë°”ë¡œ ëŠê¸¸ìˆ˜ë¡ ë¶ˆë¦¬ ----
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)  # 0~1
        # threshold ì´í•˜ë©´ ì„ í˜• íŒ¨ë„í‹°, ì´ìƒì´ë©´ 0
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        total -= (w_j * jitter_pen)

        # ---- Ping-pong proxy: ì§ì „ ë…¸ë“œ(prev_node)ë¡œ ë˜ëŒì•„ê°€ëŠ” A->B->A íŒ¨í„´ ----
        # prev_node --(ì´ì „ ìŠ¬ë¡¯)--> current_node --(ì´ë²ˆ ê²°ì •)--> n
        if prev_node != -1 and current_node != -1 and prev_node == n and current_node != n:
            total -= w_p

    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        inst = (w_u * u * a) - (w_o * (1.0 - a))
        if tau == i:
            inst += (w_r * robustness)
        if cfg_te.reward_use_discounted_return:
            total += (gamma ** (tau - i)) * inst
        else:
            total += inst

    return float(total)


def _find_best_reward_guided_target(
    future_A_true: np.ndarray,    # (H, N)
    future_U_true: np.ndarray,    # (H, N)
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> Tuple[int, int, float]:
    """
    ë¯¸ë˜ H-step í›„ë³´ (time_offset, node)ë¥¼ ìŠ¤ìº”í•´ì„œ í†µí•©ë³´ìƒ(mini) ê¸°ì¤€ ìµœì  í›„ë³´ë¥¼ ì„ íƒ.
    ìœ íš¨ í›„ë³´ê°€ ì—†ìœ¼ë©´ (0,0, invalid_penalty) ë°˜í™˜.
    """
    H, N = future_A_true.shape
    best_i, best_n = 0, 0
    best_r = -1e18

    any_valid = False
    for i in range(H):
        valid_nodes = np.where(future_A_true[i] == 1)[0]
        if len(valid_nodes) == 0:
            continue
        any_valid = True
        for n in valid_nodes:
            r = _discounted_integrated_reward_single_aircraft(
                future_A_true=future_A_true,
                future_U_true=future_U_true,
                target_time_offset=i,
                target_node_idx=int(n),
                current_node=current_node,
                cfg_te=cfg_te,
            )
            if r > best_r:
                best_r = float(r)
                best_i, best_n = int(i), int(n)

    if not any_valid:
        return 0, 0, float(cfg_te.reward_invalid_penalty)

    return best_i, best_n, float(best_r)


def _reward_to_supervision_weight(reward_val: float, cfg_te: TrainEvalRealtimeConfig) -> float:
    """
    ë³´ìƒê°’ì„ ì•ˆì •ì ì¸ supervised RL-loss ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜.
    """
    w = 1.0 + cfg_te.reward_guided_target_scale * math.tanh(reward_val / 2.0)
    w = float(np.clip(w, cfg_te.reward_guided_target_min_weight, cfg_te.reward_guided_target_max_weight))
    return w


# =========================================================
# Training (predicted history -> actual future target)
# =========================================================

def train_on_seed_pool(
    env_pairs: List[Dict[str, Any]],
    cfg_te: TrainEvalRealtimeConfig,
):
    device = get_device()
    print(f"ğŸš€ training start | device={device}")

    # num_nodes ê³ ì •
    first = env_pairs[0]
    _, N = first["actual_env"].A_final.shape

    L, H = cfg_te.L, cfg_te.H
    transformer = OnlineTransformerPredictor(num_nodes=N, L=L, H=H).to(device)
    online_consistency = OnlineConsistencyGenerator(num_nodes=N, H=H).to(device)

    target_consistency = copy.deepcopy(online_consistency).to(device)
    target_consistency.eval()
    for p in target_consistency.parameters():
        p.requires_grad = False

    optimizer = optim.Adam(
        list(transformer.parameters()) + list(online_consistency.parameters()),
        lr=cfg_te.lr,
    )
    bce_loss_fn = nn.BCELoss()

    history_rows = []
    t0 = time.time()

    for epoch in range(cfg_te.epochs):
        transformer.train()
        online_consistency.train()

        epoch_loss = 0.0
        epoch_loss_tf = 0.0
        epoch_loss_rl = 0.0
        epoch_loss_cs = 0.0
        epoch_reward = 0.0
        step_count = 0

        for pair in env_pairs:
            actual_env = pair["actual_env"]
            pred_env = pair["pred_env"]

            A_in = np.asarray(pred_env.A_final, dtype=np.float32)        # ì…ë ¥ = predicted
            A_target = np.asarray(actual_env.A_final, dtype=np.float32)  # íƒ€ê¹ƒ = actual
            U_target = np.asarray(actual_env.utility, dtype=np.float32)  # ë³´ìƒ = actual utility

            T, N2 = A_in.shape
            if N2 != N:
                raise ValueError(f"num_nodes mismatch in training: expected {N}, got {N2}")

            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
            for t in range(L, T - H, cfg_te.stride):
                history_A = A_in[t - L : t, :]
                future_A_true = A_target[t : t + H, :]

                state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)
                future_tensor_true = torch.tensor(future_A_true, dtype=torch.float32, device=device).unsqueeze(0)

                optimizer.zero_grad()

                # 1) Transformer: predicted history -> actual future coverage
                future_pred = transformer(state_tensor)
                loss_tf = bce_loss_fn(future_pred, future_tensor_true)

                # 2) Consistency action generation (continuous target: [time_ratio, node_ratio])
                condition = future_pred.view(1, -1).detach()
                y_noisy_t2 = torch.randn(1, 2, device=device)
                y_noisy_t1 = y_noisy_t2 * 0.5

                step_2 = torch.tensor([[2.0]], device=device)
                step_1 = torch.tensor([[1.0]], device=device)

                y_curr = online_consistency(y_noisy_t1, condition, step_1)

                # 3) Reward-guided target (mini integrated reward, single-aircraft)
                # í˜„ì¬/ì§ì „ ë…¸ë“œ ê·¼ì‚¬: t-1, t-2 ìŠ¬ë¡¯ì—ì„œ actual utilityê°€ ìµœëŒ€ì¸ ê°€ìš© ë…¸ë“œ
                if t - 1 >= 0:
                    current_node_approx = _best_available_node(A_target[t - 1], U_target[t - 1])
                else:
                    current_node_approx = -1
                if t - 2 >= 0:
                    prev_node_approx = _best_available_node(A_target[t - 2], U_target[t - 2])
                else:
                    prev_node_approx = -1

                best_i, best_n, reward_val = _find_best_reward_guided_target(
                    future_A_true=future_A_true,
                    future_U_true=U_target[t : t + H, :],
                    current_node=current_node_approx,
                    cfg_te=cfg_te,
                    prev_node=prev_node_approx,
                )
                epoch_reward += reward_val

                # reward-guided normalized target y* = [time_ratio, node_ratio]
                y_star = torch.tensor(
                    [[
                        0.0 if H <= 1 else (best_i / float(H - 1)),
                        0.0 if N <= 1 else (best_n / float(N - 1)),
                    ]],
                    dtype=torch.float32,
                    device=device,
                )
                rl_weight = _reward_to_supervision_weight(reward_val, cfg_te)
                loss_rl = rl_weight * F.mse_loss(y_curr, y_star)

                # 4) Consistency self-consistency loss (EMA target)
                with torch.no_grad():
                    y_target = target_consistency(y_noisy_t2, condition, step_2)
                loss_cs = F.mse_loss(y_curr, y_target)

                total_loss = loss_tf + loss_rl + (cfg_te.lambda_consistency * loss_cs)
                total_loss.backward()
                optimizer.step()
                update_ema_target(online_consistency, target_consistency)

                epoch_loss += float(total_loss.item())
                epoch_loss_tf += float(loss_tf.item())
                epoch_loss_rl += float(loss_rl.item())
                epoch_loss_cs += float(loss_cs.item())
                step_count += 1

        row = {
            "epoch": epoch + 1,
            "steps": step_count,
            "avg_total_loss": epoch_loss / max(1, step_count),
            "avg_tf_loss": epoch_loss_tf / max(1, step_count),
            "avg_rl_loss": epoch_loss_rl / max(1, step_count),
            "avg_cs_loss": epoch_loss_cs / max(1, step_count),
            "avg_reward": epoch_reward / max(1, step_count),
            "elapsed_sec": time.time() - t0,
        }
        history_rows.append(row)
        print(
            f"[train] epoch {epoch+1:02d}/{cfg_te.epochs} | "
            f"loss={row['avg_total_loss']:.4f} (tf={row['avg_tf_loss']:.4f}, rl={row['avg_rl_loss']:.4f}, cs={row['avg_cs_loss']:.4f}) | "
            f"reward={row['avg_reward']:.4f} | elapsed={row['elapsed_sec']:.1f}s"
        )

    transformer.eval()
    online_consistency.eval()

    save_weights(transformer, online_consistency, cfg_te.weight_path)

    hist_df = pd.DataFrame(history_rows)
    hist_df.to_csv(Path(cfg_te.results_dir) / "train_history_real_env.csv", index=False)
    print(f"âœ… train history saved: {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")

    return transformer, online_consistency, hist_df


# =========================================================
# Inference helpers (learned planner)
# =========================================================

def _predict_target_from_history(
    transformer,
    consistency,
    history_A: np.ndarray,
    H: int,
    N: int,
    device: torch.device,
    consistency_steps: int = 2,
) -> Tuple[int, int]:
    """
    history_A: (L, N) binary matrix
    return: (target_time_offset, target_node_idx)
    """
    state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)

    with torch.no_grad():
        future_pred = transformer(state_tensor)
        condition = future_pred.view(1, -1)

        y_curr = torch.randn(1, 2, device=device)

        # configurable few-step consistency
        steps = max(1, int(consistency_steps))
        for s in range(steps, 0, -1):
            step_tensor = torch.tensor([[float(s)]], dtype=torch.float32, device=device)
            y_curr = consistency(y_curr, condition, step_tensor)

        target_time_offset = int(y_curr[0, 0].item() * (H - 1))
        target_node_idx = int(y_curr[0, 1].item() * (N - 1))

        target_time_offset = min(max(target_time_offset, 0), H - 1)
        target_node_idx = min(max(target_node_idx, 0), N - 1)

    return target_time_offset, target_node_idx


def build_learned_offline_plan_from_pred_env(
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    predicted envë§Œ ë³´ê³  ë§Œë“  learned plan (offline planning)
    """
    device = next(transformer.parameters()).device
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)
    T, N = A_pred.shape

    L, H = cfg_te.L, cfg_te.H
    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1

    for t in range(T):
        if t < L:
            available = np.where(A_pred[t] == 1)[0]
            if len(available) > 0:
                current_node = int(available[np.argmax(U_pred[t, available])])
            else:
                current_node = -1
            planned_idx[t] = current_node
            continue

        history_A = A_pred[t - L : t, :]
        target_time_offset, target_node_idx = _predict_target_from_history(
            transformer=transformer,
            consistency=consistency,
            history_A=history_A,
            H=H,
            N=N,
            device=device,
            consistency_steps=consistency_steps,
        )

        # í˜„ì¬ ì‹œì  íƒ€ì´ë°ì´ë©´ switch ì œì•ˆ
        if target_time_offset == 0 and A_pred[t, target_node_idx] == 1:
            current_node = int(target_node_idx)

        # predicted viewì—ì„œ ì´ë¯¸ ëŠê¸´ ê²½ìš°
        if current_node != -1 and A_pred[t, current_node] == 0:
            current_node = -1

        planned_idx[t] = current_node

    return planned_idx


def build_learned_realtime_corrected_plan(
    actual_env,
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    ì‹¤ì‹œê°„ ë³´ì • ë²„ì „ (receding-horizon, MPC ìŠ¤íƒ€ì¼):
    - ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ predicted historyë¥¼ ì‚¬ìš©
    - ì§€ë‚˜ê°„ ìŠ¬ë¡¯ì€ ì‹¤ì œ ê´€ì¸¡(actual)ë¡œ ë®ì–´ì”€ (observed prefix correction)
    - ìŠ¬ë¡¯ë§ˆë‹¤ node reliability EMA ê°±ì‹ 
    - ì €ì‹ ë¢°/ë¹„ê°€ìš© ì œì•ˆì€ í˜„ì¬ ì‹¤ì œ ê°€ìš© í›„ë³´ë¡œ fallback
    - (ì˜µì…˜) HO ì•ˆì •í™” ê°€ë“œë ˆì¼: dwell + hysteresis + ping-pong ì–µì œ
      -> ê³¼ë„í•œ HO/í•‘íìœ¼ë¡œ latency/HO attemptsê°€ í­ì¦í•˜ëŠ” í˜„ìƒì„ ì™„í™”
    """
    device = next(transformer.parameters()).device

    A_actual = np.asarray(actual_env.A_final)
    U_actual = np.asarray(actual_env.utility)
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)

    T, N = A_actual.shape
    L, H = cfg_te.L, cfg_te.H

    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1

    # ê³¼ê±° ê´€ì¸¡ ëˆ„ì  ë°˜ì˜ìš© (ì²˜ìŒì—” predictedë¡œ ì‹œì‘, ì§€ë‚˜ê°„ ìŠ¬ë¡¯ì€ actualë¡œ ë®ì–´ì”€)
    A_obs_corrected = A_pred.copy()
    # node ì‹ ë¢°ë„ (0~1)
    reliability = np.ones(N, dtype=np.float32)

    # ë§ˆì§€ë§‰ ìŠ¤ìœ„ì¹˜ ì‹œê°(ìŠ¬ë¡¯ ì¸ë±ìŠ¤)
    last_switch_t = -10**9

    def _score(t_: int, n_: int) -> float:
        # utility * reliability (ë‘˜ ë‹¤ 0~ëŒ€ëµ 1 scale ê¸°ëŒ€)
        return float(U_actual[t_, n_] * reliability[n_])

    for t in range(T):
        # í˜„ì¬ ìŠ¬ë¡¯ê¹Œì§€ëŠ” ì‹¤ì œ ê´€ì¸¡ê°’ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •(ì‹¤ì‹œê°„ ì¸¡ì •)
        A_obs_corrected[t] = A_actual[t]

        prev1 = int(planned_idx[t - 1]) if t - 1 >= 0 else -1
        prev2 = int(planned_idx[t - 2]) if t - 2 >= 0 else -1

        # í˜„ì¬ ì—°ê²°ì˜ ìœ íš¨ì„±(í˜„ì¬ ìŠ¬ë¡¯ì—ì„œ ì»¤ë²„ë¦¬ì§€/ì •ì±…ìƒ ê°€ëŠ¥?)
        current_valid_now = (current_node >= 0 and current_node < N and A_actual[t, current_node] == 1)

        # ------------------------ (A) ê¸°ë³¸ ì œì•ˆ ì‚°ì¶œ ------------------------
        if t < L:
            # warmup: í˜„ì¬ ì‹¤ì œ ê°€ìš© í›„ë³´ ì¤‘ ì‹ ë¢°ë„ ë°˜ì˜ ì ìˆ˜ ìµœëŒ€ ì„ íƒ
            avail = np.where(A_actual[t] == 1)[0]
            if len(avail) > 0:
                score = U_actual[t, avail] * reliability[avail]
                proposed = int(avail[np.argmax(score)])
            else:
                proposed = -1

            corrected = proposed

        else:
            # historyëŠ” "ê³¼ê±° actual ê´€ì¸¡ ë°˜ì˜ + (ë¯¸ë˜ëŠ” ì—¬ì „íˆ predicted)"
            history_A = A_obs_corrected[t - L : t, :]

            target_time_offset, target_node_idx = _predict_target_from_history(
                transformer=transformer,
                consistency=consistency,
                history_A=history_A,
                H=H,
                N=N,
                device=device,
                consistency_steps=consistency_steps,
            )

            proposed = current_node
            if target_time_offset == 0:
                proposed = int(target_node_idx)

            # ì‹¤ì‹œê°„ ë³´ì • ë¡œì§ (ê¸°ì¡´)
            proposed_valid_now = (proposed is not None and proposed >= 0 and proposed < N and A_actual[t, proposed] == 1)

            low_rel = False
            if proposed is not None and proposed >= 0 and proposed < N:
                low_rel = (float(reliability[proposed]) < cfg_te.rt_low_reliability_threshold)

            corrected = current_node

            if proposed_valid_now and not low_rel:
                corrected = proposed
            else:
                # proposedê°€ ì§€ê¸ˆ ì•ˆë˜ê±°ë‚˜ / ì‹ ë¢°ë„ ë„ˆë¬´ ë‚®ìœ¼ë©´ fallback
                if cfg_te.rt_use_actual_current_slot_fallback:
                    avail = np.where(A_actual[t] == 1)[0]
                    if len(avail) > 0:
                        # ì‹ ë¢°ë„ ë°˜ì˜ + í˜„ì¬ actual utility ê¸°ë°˜ ì¦‰ì‹œ ë³´ì •
                        score = U_actual[t, avail] * reliability[avail]
                        fallback = int(avail[np.argmax(score)])

                        # í˜„ì¬ ì—°ê²° ìœ ì§€ê°€ ìœ íš¨í•˜ê³  ì œì•ˆì´ low_relì´ë©´ ìœ ì§€ë¥¼ ìš°ì„ í•  ìˆ˜ë„ ìˆìŒ
                        if current_valid_now and low_rel:
                            corrected = current_node
                        else:
                            corrected = fallback
                    else:
                        corrected = -1 if not current_valid_now else current_node
                else:
                    corrected = current_node if current_valid_now else -1

        # ------------------------ (B) HO ì•ˆì •í™” ê°€ë“œë ˆì¼ ------------------------
        if cfg_te.rt_enable_guardrails:
            # ìµœì¢… í›„ë³´ ìœ íš¨ì„±
            corrected_valid_now = (corrected is not None and corrected >= 0 and corrected < N and A_actual[t, corrected] == 1)

            # ìŠ¤ìœ„ì¹˜ ì‹œë„ì¸ì§€?
            switching = (corrected_valid_now and current_valid_now and corrected != current_node)

            if switching:
                # (1) Dwell: ìµœê·¼ ìŠ¤ìœ„ì¹˜ í›„ ì¼ì • ì‹œê°„ì€ ìœ ì§€
                if (t - last_switch_t) < int(cfg_te.rt_min_dwell):
                    corrected = current_node
                else:
                    # (2) Hysteresis: ì¶©ë¶„í•œ ì´ë“ì´ ìˆì„ ë•Œë§Œ ìŠ¤ìœ„ì¹˜
                    s_cur = _score(t, current_node)
                    s_new = _score(t, corrected)

                    required_ratio = float(cfg_te.rt_hysteresis_ratio)

                    # (3) Ping-pong(A->B->A) ì–µì œ: t-2 ë…¸ë“œë¡œ ë˜ëŒì•„ê°€ë ¤ í•˜ë©´ ë” í° íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ìš”êµ¬
                    is_pingpong = (corrected == prev2 and prev2 != -1 and prev1 == current_node and (t - last_switch_t) <= int(cfg_te.rt_pingpong_window))
                    if is_pingpong:
                        required_ratio += float(cfg_te.rt_pingpong_extra_hysteresis_ratio)

                    ratio_ok = (s_new >= s_cur * (1.0 + required_ratio))
                    if float(cfg_te.rt_hysteresis_abs) > 0.0:
                        abs_ok = ((s_new - s_cur) >= float(cfg_te.rt_hysteresis_abs))
                        ok = ratio_ok and abs_ok
                    else:
                        ok = ratio_ok

                    if not ok:
                        corrected = current_node

            # í˜„ì¬ ì—°ê²°ì´ ìœ íš¨í•œë° correctedê°€ -1ì´ë©´ ëŠê¹€ ë°©ì§€ë¡œ ìœ ì§€(ë‹¨, ì‹¤ì œë¡œ ìœ íš¨í•  ë•Œë§Œ)
            if corrected == -1 and current_valid_now:
                corrected = current_node

        # ------------------------ (C) ìµœì¢… ì ìš© & ìŠ¤ìœ„ì¹˜ ê¸°ë¡ ------------------------
        # ìµœì¢… ìœ íš¨ì„± ì¬ê²€ì‚¬
        if corrected != -1 and (corrected < 0 or corrected >= N or A_actual[t, corrected] != 1):
            corrected = -1

        # ìŠ¤ìœ„ì¹˜ ë°œìƒ ì‹œê° ê¸°ë¡ (ì—°ê²°ì´ ìœ íš¨í•œ ìƒíƒœì—ì„œ ë‹¤ë¥¸ ìœ íš¨ ë…¸ë“œë¡œ ë°”ë€ ê²½ìš°)
        if corrected != current_node and corrected != -1:
            last_switch_t = t

        current_node = int(corrected) if corrected is not None else -1
        planned_idx[t] = current_node

        # ---- reliability update (í˜„ì¬ ìŠ¬ë¡¯ ê´€ì¸¡ìœ¼ë¡œ ë‹¤ìŒ ìŠ¬ë¡¯ planning ë³´ì •ì— ë°˜ì˜) ----
        match = (A_pred[t].astype(np.int32) == A_actual[t].astype(np.int32)).astype(np.float32)
        reliability = cfg_te.rt_reliability_alpha * reliability + (1.0 - cfg_te.rt_reliability_alpha) * match

        # false positive ì˜ˆì¸¡(ì˜ˆì¸¡=1, ì‹¤ì œ=0)ì—ëŠ” ì¶”ê°€ í˜ë„í‹°
        fp_mask = (A_pred[t] == 1) & (A_actual[t] == 0)
        reliability[fp_mask] *= cfg_te.rt_fp_extra_penalty

        reliability = np.clip(reliability, 0.05, 1.0)

    return planned_idx


# =========================================================
# Evaluation on hold-out seeds
# =========================================================

def evaluate_on_test_seeds(
    test_pairs: List[Dict[str, Any]],
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    rows = []
    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    print("\n=== Evaluation: Learned planner on held-out seeds ===")
    for pair in test_pairs:
        seed = pair["seed"]
        actual_env = pair["actual_env"]
        pred_env = pair["pred_env"]

        np.random.seed(seed)  # runtime miss stochasticity ì¬í˜„ì„± í™•ë³´

        # 1) Baseline: sustainable DAG on actual env
        dag_plan = sustainable_dag_plan(actual_env, cfg_te.exp_cfg)
        tl_dag, m_dag = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=dag_plan,
            method_name="Sustainable_DAG_NetworkX",
            mode="sustainable_dag",
            sampling_steps=0,
            cfg=cfg_te.exp_cfg,
        )
        m_dag["seed"] = seed
        rows.append(m_dag)

        # 2) Learned offline (predicted env only)
        offline_plan = build_learned_offline_plan_from_pred_env(
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_off, m_off = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=offline_plan,
            method_name="Learned_TC_Offline",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_off["seed"] = seed
        rows.append(m_off)

        # 3) Learned + realtime correction
        rt_plan = build_learned_realtime_corrected_plan(
            actual_env=actual_env,
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_rt, m_rt = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=rt_plan,
            method_name="Learned_TC_RTCorrected",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_rt["seed"] = seed
        rows.append(m_rt)

        print(
            f"[seed={seed}]\n"
            f" ğŸ”¹ [DAG] Avail: {m_dag['Availability']:.4f} | QoE: {m_dag['EffectiveQoE']:.4f} | í•‘í: {m_dag['PingPong_Ratio']:.4f} | HOì‹¤íŒ¨: {m_dag['HO_Failure_Ratio']:.4f}\n"
            f" ğŸ”¸ [ AI] Avail: {m_rt['Availability']:.4f} | QoE: {m_rt['EffectiveQoE']:.4f} | í•‘í: {m_rt['PingPong_Ratio']:.4f} | HOì‹¤íŒ¨: {m_rt['HO_Failure_Ratio']:.4f}\n"
            f" --------------------------------------------------------"
        )

        # seed0 timeline ì €ì¥ (test ì²« seed)
        if seed == test_pairs[0]["seed"]:
            tl_dag.to_csv(results_dir / f"timeline_sustainable_dag_seed{seed}.csv", index=False)
            tl_off.to_csv(results_dir / f"timeline_learned_tc_offline_seed{seed}.csv", index=False)
            tl_rt.to_csv(results_dir / f"timeline_learned_tc_rtcorrected_seed{seed}.csv", index=False)

    df_runs = pd.DataFrame(rows)
    df_summary = summarize_runs(df_runs)

    df_runs.to_csv(results_dir / "exp3_learned_realtime_runs.csv", index=False)
    df_summary.to_csv(results_dir / "exp3_learned_realtime_summary.csv", index=False)

    print(f"âœ… saved: {results_dir / 'exp3_learned_realtime_runs.csv'}")
    print(f"âœ… saved: {results_dir / 'exp3_learned_realtime_summary.csv'}")
    return df_runs, df_summary


# =========================================================
# Main pipeline
# =========================================================

def main():
    cfg_te = TrainEvalRealtimeConfig()

    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    # experiments_atg_leoì˜ env builderê°€ ì“°ëŠ” TLE íŒŒì¼ í™•ì¸
    if not Path(cfg_te.exp_cfg.tle_path).exists():
        raise FileNotFoundError(
            f"Frozen TLE file not found: {cfg_te.exp_cfg.tle_path}\n"
            "ë¨¼ì € TLE íŒŒì¼ ê²½ë¡œë¥¼ ë§ì¶°ì£¼ì„¸ìš”."
        )

    # split ì €ì¥
    split_manifest = {
        "train_seeds": cfg_te.train_seeds,
        "val_seeds": cfg_te.val_seeds,
        "test_seeds": cfg_te.test_seeds,
    }
    with open(cfg_te.split_manifest_path, "w", encoding="utf-8") as f:
        json.dump(split_manifest, f, ensure_ascii=False, indent=2)

    print("=== Seed Split ===")
    print(json.dumps(split_manifest, ensure_ascii=False, indent=2))

    t0 = time.time()

    # 1) collect
    train_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.train_seeds)
    val_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.val_seeds)   # ì§€ê¸ˆì€ ë¡œê¹…ìš©/í™•ì¥ìš©
    test_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.test_seeds)

    print(f"[collect done] train={len(train_pairs)}, val={len(val_pairs)}, test={len(test_pairs)}")

    # 2) train
    transformer, consistency, train_hist = train_on_seed_pool(train_pairs, cfg_te)

    # (ì„ íƒ) val quick sanity metricsë¥¼ ì›í•˜ë©´ ì—¬ê¸° ì¶”ê°€ ê°€ëŠ¥
    # ì§€ê¸ˆì€ ë°”ë¡œ test í‰ê°€

    # 3) evaluate (hold-out)
    df_runs, df_summary = evaluate_on_test_seeds(
        test_pairs=test_pairs,
        transformer=transformer,
        consistency=consistency,
        cfg_te=cfg_te,
    )

    elapsed = time.time() - t0
    print("\n=== Done ===")
    print(f"Total elapsed: {elapsed:.2f} sec")
    print("Outputs:")
    print(f" - {cfg_te.weight_path}")
    print(f" - {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_runs.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_summary.csv'}")
    print(f" - {cfg_te.split_manifest_path}")

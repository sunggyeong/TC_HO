#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
paper_pkg.tc_train_eval (CLEAN)

Core training/evaluation module for Transformer + Consistency (TC), with realtime guardrails.

This is the import-safe version:
- NO CLI main()
- Imports env/execution from paper_pkg.env_core
- Used by paper_pkg.train and paper_pkg.eval
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from types import SimpleNamespace
from typing import List, Tuple, Dict, Any
import copy
import json
import time
import math

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# === 기존 실험 파일에서 재사용 ===
from .env_core import (
    ExperimentConfig,
    build_env,
    execute_plan_on_env,
    sustainable_dag_plan,
    summarize_runs,
)

# === 모델 import (프로젝트 구조에 따라 둘 중 하나가 맞을 수 있음) ===
try:
    from models.proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator
except ImportError:
    from proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator


# =========================================================
# Config
# =========================================================

@dataclass
class TrainEvalRealtimeConfig:
    exp_cfg: ExperimentConfig = field(default_factory=ExperimentConfig)

    # 데이터 분할 (예시)
    train_seeds: List[int] = field(default_factory=lambda: [0])
    val_seeds: List[int] = field(default_factory=lambda: [1])
    test_seeds: List[int] = field(default_factory=lambda: [2])

    # 모델/학습
    L: int = 20
    H: int = 30
    epochs: int = 3
    stride: int = 5
    lr: float = 5e-4
    lambda_consistency: float = 1.0
    reward_invalid_penalty: float = -1.0

    # ---- 통합 보상(mini) 하이퍼파라미터 ----
    # (단일 항공기 시나리오이므로 LB는 제외)
    reward_gamma: float = 0.97                 # 할인계수 γ
    reward_w_utility: float = 1.00             # 가용 슬롯 utility 보상
    reward_w_outage: float = 0.70              # 비가용(outage) 패널티
    reward_w_switch: float = 0.05              # 핸드오버(스위치) 패널티
    reward_w_robustness: float = 0.20          # 후보 노드의 미래 가용성(robustness) 보너스
    reward_w_jitter: float = 0.15              # 짧은 유지시간 기반 지터 proxy 패널티
    reward_w_pingpong: float = 0.20            # A->B->A 형태 ping-pong proxy 패널티
    reward_jitter_min_hold_ratio: float = 0.35 # 이 비율보다 짧게 유지되면 지터 패널티 강화
    reward_use_discounted_return: bool = True  # 할인 누적보상 사용 여부
    reward_guided_target_scale: float = 1.0    # RL-loss 가중 스케일 (보상 기반)
    reward_guided_target_min_weight: float = 0.1
    reward_guided_target_max_weight: float = 3.0

    # 설계안 B: "일찍 스위치" 유도 — i가 클수록 보상에서 추가 패널티
    reward_time_offset_penalty: float = 0.02   # total -= penalty * i (i=0 선호)
    reward_tie_breaker_epsilon: float = 1e-6    # 보상 차이 < eps면 같은 것으로 보고 더 작은 i 선호

    # 추론: target_time_offset <= k 이면 "지금/곧 스위치"로 해석 (기본 1 = 0 또는 1 슬롯 안)
    rt_switch_time_offset_max: int = 1

    # 학습 중 보상 가중치 학습 여부 (True면 w_utility, w_outage 등이 nn.Parameter로 최적화)
    reward_learnable: bool = False
    alpha_oracle_loss: float = 0.5
    beta_pred_action_loss: float = 1.5
    rollout_steps: int = 8
    rollout_loss_weight: float = 1.0
    train_use_history_augmentation: bool = True
    train_aug_flip_prob: float = 0.02
    train_aug_dropout_prob: float = 0.06
    train_aug_phase_extra: float = 0.1

    # 평가 시 consistency sampling steps
    consistency_steps_eval: int = 2

    # 실시간 보정 (online correction)
    rt_reliability_alpha: float = 0.90  # EMA: 클수록 느리게 변함
    rt_low_reliability_threshold: float = 0.45
    rt_fp_extra_penalty: float = 0.90    # false positive(예측=1 실제=0) 추가 감쇠
    rt_use_actual_current_slot_fallback: bool = True


    # ---- 실시간 HO 안정화 가드레일(핑퐁/과도한 HO 억제) ----
    # enable_guardrails=False로 두면 기존 동작(매 슬롯 즉시 보정/스위치)을 그대로 유지
    rt_enable_guardrails: bool = True

    # (1) 최소 유지시간: 최근 스위치 후 rt_min_dwell 슬롯 동안은 스위치 금지(현재 연결이 유효한 경우)
    rt_min_dwell: int = 3

    # (2) 히스테리시스: 스위치하려면 현재 대비 충분한 이득이 있어야 함
    # - ratio 기준: score_new >= score_cur * (1 + rt_hysteresis_ratio)
    # - abs 기준: (선택) score_new - score_cur >= rt_hysteresis_abs
    rt_hysteresis_ratio: float = 0.08
    rt_hysteresis_abs: float = 0.0

    # (3) 핑퐁(A->B->A) 억제: 최근 rt_pingpong_window 슬롯 내 되돌림이면 더 큰 히스테리시스 요구
    rt_pingpong_window: int = 4
    rt_pingpong_extra_hysteresis_ratio: float = 0.12

    # ---- Realtime planning enhancements ----
    rt_enable_pending: bool = True
    rt_fallback_mode: str = "lookahead"  # myopic | lookahead
    rt_fallback_alpha_latency: float = 0.05
    rt_fallback_gamma: float = 0.97
    rt_fallback_H: int = 30
    rt_debug_log: bool = False

    # 저장
    results_dir: str = "results"
    weight_path: str = "results/trained_weights_rl_real_env.pth"
    split_manifest_path: str = "results/train_val_test_seed_split.json"


# =========================================================
# Utilities
# =========================================================

def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def update_ema_target(online_net, target_net, tau=0.05):
    with torch.no_grad():
        for online_param, target_param in zip(online_net.parameters(), target_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)


def save_weights(transformer, consistency, weight_path: str, reward_weights_module=None):
    Path(weight_path).parent.mkdir(parents=True, exist_ok=True)
    payload = {
        "transformer_state_dict": transformer.state_dict(),
        "consistency_state_dict": consistency.state_dict(),
        "num_nodes": getattr(transformer, "num_nodes", None),
        "L": getattr(transformer, "L", None),
        "H": getattr(transformer, "H", None),
    }
    if reward_weights_module is not None:
        payload["reward_weights_state_dict"] = reward_weights_module.state_dict()
    torch.save(payload, weight_path)
    print(f"✅ weights saved: {weight_path}")


def load_weights(transformer, consistency, weight_path: str, device: torch.device):
    ckpt = torch.load(weight_path, map_location=device)
    transformer.load_state_dict(ckpt["transformer_state_dict"])
    consistency.load_state_dict(ckpt["consistency_state_dict"])


# =========================================================
# Seed collection (actual / predicted env pair)
# =========================================================

def collect_env_pairs(cfg: ExperimentConfig, seeds: List[int]) -> List[Dict[str, Any]]:
    pairs = []
    for seed in seeds:
        print(f"[collect] seed={seed}")
        actual_env = build_env(cfg, seed=seed, predicted_view=False)
        pred_env = build_env(cfg, seed=seed, predicted_view=True)

        # 기본 shape 체크
        if actual_env.A_final.shape != pred_env.A_final.shape:
            raise ValueError(f"shape mismatch at seed={seed}: actual {actual_env.A_final.shape} vs pred {pred_env.A_final.shape}")
        if actual_env.A_final.shape[1] != pred_env.A_final.shape[1]:
            raise ValueError(f"N mismatch at seed={seed}")

        pairs.append(
            {
                "seed": seed,
                "actual_env": actual_env,
                "pred_env": pred_env,
            }
        )
    return pairs



# =========================================================
# Learnable reward weights (optional)
# =========================================================

def _inverse_softplus(y: float) -> float:
    """softplus(x)=y => x = log(exp(y)-1). y>0."""
    if y <= 0:
        return -2.0
    return float(math.log(math.exp(y) - 1.0))


class LearnableRewardWeights(nn.Module):
    """보상 가중치 6개를 nn.Parameter로 두고 softplus로 양수 유지."""
    def __init__(self, cfg_te: TrainEvalRealtimeConfig, device: torch.device):
        super().__init__()
        defaults = (
            cfg_te.reward_w_utility,
            getattr(cfg_te, "reward_w_outage", 0.7),
            getattr(cfg_te, "reward_w_switch", 0.05),
            getattr(cfg_te, "reward_w_robustness", 0.2),
            getattr(cfg_te, "reward_w_jitter", 0.15),
            getattr(cfg_te, "reward_w_pingpong", 0.2),
        )
        init_vals = [_inverse_softplus(float(d)) for d in defaults]
        self.log_weights = nn.Parameter(torch.tensor(init_vals, dtype=torch.float32, device=device))

    def forward(self) -> torch.Tensor:
        return F.softplus(self.log_weights)  # (6,) w_u, w_o, w_s, w_r, w_j, w_p

    def to_numpy_weights(self) -> Tuple[float, float, float, float, float, float]:
        w = self.forward().detach().cpu().numpy()
        return (float(w[0]), float(w[1]), float(w[2]), float(w[3]), float(w[4]), float(w[5]))


# =========================================================
# Reward helpers (mini integrated reward for single-aircraft)
# =========================================================

def _best_available_node(A_row: np.ndarray, U_row: np.ndarray) -> int:
    avail = np.where(A_row == 1)[0]
    if len(avail) == 0:
        return -1
    return int(avail[np.argmax(U_row[avail])])


def _contiguous_available_run_len(future_A_true: np.ndarray, start_i: int, node_idx: int) -> int:
    """start_i부터 연속 가용(1)인 길이. 첫 슬롯이 0이면 0."""
    H, _ = future_A_true.shape
    run_len = 0
    for tau in range(int(start_i), H):
        if int(future_A_true[tau, node_idx]) != 1:
            break
        run_len += 1
    return int(run_len)


def _discounted_integrated_reward_single_aircraft(
    future_A_true: np.ndarray,   # (H, N)
    future_U_true: np.ndarray,   # (H, N)
    target_time_offset: int,
    target_node_idx: int,
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> float:
    """
    논문식 통합 보상의 '미니 버전' (단일 항공기):
      - utility 항 (정규화된 utility 사용)
      - outage 패널티
      - switch(핸드오버) 비용
      - robustness 보너스(선택 노드의 미래 가용 비율)
      - jitter proxy 패널티 (짧은 유지시간에 스위치할수록 불리)
      - ping-pong proxy 패널티 (A->B->A)
    ※ Load Balance(LB)는 단일 항공기라 제외
    """
    H, N = future_A_true.shape
    i = int(np.clip(target_time_offset, 0, H - 1))
    n = int(np.clip(target_node_idx, 0, N - 1))

    gamma = float(cfg_te.reward_gamma)
    w_u = float(cfg_te.reward_w_utility)
    w_o = float(cfg_te.reward_w_outage)
    w_s = float(cfg_te.reward_w_switch)
    w_r = float(cfg_te.reward_w_robustness)
    w_j = float(getattr(cfg_te, "reward_w_jitter", 0.0))
    w_p = float(getattr(cfg_te, "reward_w_pingpong", 0.0))
    jitter_thr = float(getattr(cfg_te, "reward_jitter_min_hold_ratio", 0.35))

    if i >= H:
        return float(cfg_te.reward_invalid_penalty)

    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    total = 0.0

    # 선택 시점에 1회 스위치 비용
    switch_happens = bool(current_node != -1 and current_node != n)
    if switch_happens:
        total -= w_s

        # ---- Jitter proxy: 스위치했는데 곧바로 끊길수록 불리 ----
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)  # 0~1
        # threshold 이하면 선형 패널티, 이상이면 0
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        total -= (w_j * jitter_pen)

        # ---- Ping-pong proxy: 직전 노드(prev_node)로 되돌아가는 A->B->A 패턴 ----
        # prev_node --(이전 슬롯)--> current_node --(이번 결정)--> n
        if prev_node != -1 and current_node != -1 and prev_node == n and current_node != n:
            total -= w_p

    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        inst = (w_u * u * a) - (w_o * (1.0 - a))
        if tau == i:
            inst += (w_r * robustness)
        if cfg_te.reward_use_discounted_return:
            total += (gamma ** (tau - i)) * inst
        else:
            total += inst

    # 설계안 B: i가 클수록 패널티 (일찍 스위치 유도)
    total -= float(getattr(cfg_te, "reward_time_offset_penalty", 0.0)) * i
    return float(total)


def _discounted_integrated_reward_with_weights(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    target_time_offset: int,
    target_node_idx: int,
    current_node: int,
    prev_node: int,
    w_u: float, w_o: float, w_s: float, w_r: float, w_j: float, w_p: float,
    gamma: float, jitter_thr: float, use_discounted: bool,
    time_offset_penalty: float = 0.0,
) -> float:
    """보상 계산 (가중치를 외부에서 주입). 학습 가능 가중치용."""
    H, N = future_A_true.shape
    i = int(np.clip(target_time_offset, 0, H - 1))
    n = int(np.clip(target_node_idx, 0, N - 1))
    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    total = 0.0
    switch_happens = bool(current_node != -1 and current_node != n)
    if switch_happens:
        total -= w_s
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        total -= w_j * jitter_pen
        if prev_node != -1 and current_node != -1 and prev_node == n and current_node != n:
            total -= w_p
    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        inst = (w_u * u * a) - (w_o * (1.0 - a))
        if tau == i:
            inst += w_r * robustness
        total += (gamma ** (tau - i)) * inst if use_discounted else inst
    total -= time_offset_penalty * i
    return float(total)


def _find_best_reward_guided_target(
    future_A_true: np.ndarray,    # (H, N)
    future_U_true: np.ndarray,    # (H, N)
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> Tuple[int, int, float]:
    """
    미래 H-step 후보 (time_offset, node)를 스캔해서 통합보상(mini) 기준 최적 후보를 선택.
    유효 후보가 없으면 (0,0, invalid_penalty) 반환.
    """
    H, N = future_A_true.shape
    best_i, best_n = 0, 0
    best_r = -1e18

    any_valid = False
    for i in range(H):
        valid_nodes = np.where(future_A_true[i] == 1)[0]
        if len(valid_nodes) == 0:
            continue
        any_valid = True
        for n in valid_nodes:
            r = _discounted_integrated_reward_single_aircraft(
                future_A_true=future_A_true,
                future_U_true=future_U_true,
                target_time_offset=i,
                target_node_idx=int(n),
                current_node=current_node,
                cfg_te=cfg_te,
            )
            eps = float(getattr(cfg_te, "reward_tie_breaker_epsilon", 1e-6))
            if r > best_r or (abs(r - best_r) < eps and i < best_i):
                best_r = float(r)
                best_i, best_n = int(i), int(n)

    if not any_valid:
        return 0, 0, float(cfg_te.reward_invalid_penalty)

    return best_i, best_n, float(best_r)


def _find_best_reward_guided_target_with_weights(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    current_node: int,
    prev_node: int,
    w_u: float, w_o: float, w_s: float, w_r: float, w_j: float, w_p: float,
    gamma: float, jitter_thr: float, use_discounted: bool,
    invalid_penalty: float,
    time_offset_penalty: float = 0.0,
    tie_breaker_epsilon: float = 1e-6,
) -> Tuple[int, int, float]:
    """보상 가중치를 외부에서 주입하여 best (i, n) 및 보상값 반환. 학습 가능 가중치용."""
    H, N = future_A_true.shape
    best_i, best_n = 0, 0
    best_r = -1e18
    any_valid = False
    for i in range(H):
        valid_nodes = np.where(future_A_true[i] == 1)[0]
        if len(valid_nodes) == 0:
            continue
        any_valid = True
        for n in valid_nodes:
            r = _discounted_integrated_reward_with_weights(
                future_A_true, future_U_true, i, int(n), current_node, prev_node,
                w_u, w_o, w_s, w_r, w_j, w_p, gamma, jitter_thr, use_discounted,
                time_offset_penalty=time_offset_penalty,
            )
            if r > best_r or (abs(r - best_r) < tie_breaker_epsilon and i < best_i):
                best_r = r
                best_i, best_n = i, int(n)
    if not any_valid:
        return 0, 0, invalid_penalty
    return best_i, best_n, float(best_r)


def _reward_coefficients_for_action(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    best_i: int,
    best_n: int,
    current_node: int,
    prev_node: int,
    gamma: float,
    jitter_thr: float,
    use_discounted: bool,
) -> Tuple[float, float, float, float, float, float]:
    """
    R = c_u*w_u + c_o*w_o + c_s*w_s + c_r*w_r + c_j*w_j + c_p*w_p 인 계수 (c_u,...,c_p) 반환.
    학습 가능 가중치에 대한 gradient를 위해 사용.
    """
    H, N = future_A_true.shape
    i = int(np.clip(best_i, 0, H - 1))
    n = int(np.clip(best_n, 0, N - 1))
    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    switch_happens = bool(current_node != -1 and current_node != n)
    # utility 계수: sum_tau gamma^(tau-i) * u*a
    c_u = 0.0
    c_o = 0.0
    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        fac = (gamma ** (tau - i)) if use_discounted else 1.0
        c_u += fac * (u * a)
        c_o -= fac * (1.0 - a)
    c_r = robustness  # w_r * robustness added once at tau==i
    c_s = -1.0 if switch_happens else 0.0
    if switch_happens:
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        c_j = -jitter_pen
        pingpong = prev_node != -1 and current_node != -1 and prev_node == n and current_node != n
        c_p = -1.0 if pingpong else 0.0
    else:
        c_j = 0.0
        c_p = 0.0
    return (c_u, c_o, c_s, c_r, c_j, c_p)


def _reward_to_supervision_weight(reward_val: float, cfg_te: TrainEvalRealtimeConfig) -> float:
    """
    보상값을 안정적인 supervised RL-loss 가중치로 변환.
    """
    w = 1.0 + cfg_te.reward_guided_target_scale * math.tanh(reward_val / 2.0)
    w = float(np.clip(w, cfg_te.reward_guided_target_min_weight, cfg_te.reward_guided_target_max_weight))
    return w




def _reward_for_action(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    action_i: int,
    action_n: int,
    current_node: int,
    prev_node: int,
    cfg_te: TrainEvalRealtimeConfig,
) -> float:
    return _discounted_integrated_reward_single_aircraft(
        future_A_true=future_A_true,
        future_U_true=future_U_true,
        target_time_offset=action_i,
        target_node_idx=action_n,
        current_node=current_node,
        cfg_te=cfg_te,
        prev_node=prev_node,
    )


def _augment_history_matrix(history_A: np.ndarray, actual_env, t: int, cfg_te: TrainEvalRealtimeConfig) -> np.ndarray:
    aug = np.array(history_A, copy=True)
    if not getattr(cfg_te, "train_use_history_augmentation", True):
        return aug
    flip_prob = float(getattr(cfg_te, "train_aug_flip_prob", 0.02))
    drop_prob = float(getattr(cfg_te, "train_aug_dropout_prob", 0.05))
    extra = float(getattr(cfg_te, "train_aug_phase_extra", 0.05))
    ph = getattr(actual_env, "phase", None)
    if ph is not None and t > 0:
        window = ph[max(0, t - len(history_A)):t]
        if len(window) >= 2 and np.any(window[1:] != window[:-1]):
            flip_prob += extra
            drop_prob += extra
    if drop_prob > 0:
        mask = (np.random.rand(*aug.shape) < drop_prob)
        aug[mask] = 0.0
    if flip_prob > 0:
        mask = (np.random.rand(*aug.shape) < flip_prob)
        aug[mask] = 1.0 - aug[mask]
    return aug


def _soft_expected_action_loss(y_curr: torch.Tensor, future_A_true: np.ndarray, future_U_true: np.ndarray,
                               current_node: int, prev_node: int, cfg_te: TrainEvalRealtimeConfig) -> torch.Tensor:
    H = future_A_true.shape[0]
    N = future_A_true.shape[1]
    logits_t = -((torch.arange(H, device=y_curr.device, dtype=torch.float32) / max(1, H - 1)) - y_curr[0, 0]) ** 2 * 24.0
    logits_n = -((torch.arange(N, device=y_curr.device, dtype=torch.float32) / max(1, N - 1)) - y_curr[0, 1]) ** 2 * 24.0
    pt = torch.softmax(logits_t, dim=0)
    pn = torch.softmax(logits_n, dim=0)
    rewards = torch.zeros((H, N), dtype=torch.float32, device=y_curr.device)
    for i in range(H):
        for n in range(N):
            rewards[i, n] = _reward_for_action(future_A_true, future_U_true, i, n, current_node, prev_node, cfg_te)
    expected_reward = (pt[:, None] * pn[None, :] * rewards).sum()
    return -expected_reward


def _mini_rollout_penalty(transformer, online_consistency, A_in: np.ndarray, A_target: np.ndarray, U_target: np.ndarray,
                          t_start: int, current_node: int, prev_node: int, cfg_te: TrainEvalRealtimeConfig, device: torch.device) -> torch.Tensor:
    L, H = cfg_te.L, cfg_te.H
    K = max(1, int(getattr(cfg_te, 'rollout_steps', 4)))
    total = torch.tensor(0.0, device=device)
    sim_current = int(current_node)
    sim_prev = int(prev_node)
    sim_hist = np.array(A_in[max(0, t_start - L):t_start, :], copy=True)
    if sim_hist.shape[0] < L:
        pad = np.zeros((L - sim_hist.shape[0], A_in.shape[1]), dtype=np.float32)
        sim_hist = np.vstack([pad, sim_hist])
    for offs in range(K):
        t = t_start + offs
        if t >= A_target.shape[0] - H:
            break
        hist = _augment_history_matrix(sim_hist, SimpleNamespace(phase=getattr(SimpleNamespace(), 'phase', None)), t, cfg_te)
        state_tensor = torch.tensor(hist, dtype=torch.float32, device=device).unsqueeze(0)
        future_pred = transformer(state_tensor)
        condition = future_pred.view(1, -1).detach()
        y_noisy = torch.randn(1, 2, device=device)
        y_curr = online_consistency(y_noisy, condition, torch.tensor([[1.0]], device=device))
        total = total + _soft_expected_action_loss(y_curr, A_target[t:t+H, :], U_target[t:t+H, :], sim_current, sim_prev, cfg_te)
        pred_i = int(torch.clamp((y_curr[0, 0] * (H - 1)).round(), 0, H - 1).item())
        pred_n = int(torch.clamp((y_curr[0, 1] * (A_target.shape[1] - 1)).round(), 0, A_target.shape[1] - 1).item())
        if pred_i <= int(getattr(cfg_te, 'rt_switch_time_offset_max', 1)) and A_target[t, pred_n] == 1:
            sim_prev, sim_current = sim_current, pred_n
        if sim_current != -1 and A_target[t, sim_current] == 0:
            sim_prev, sim_current = sim_current, -1
        sim_hist = np.vstack([sim_hist[1:], A_in[t:t+1, :]])
    return total / float(K)


def evaluate_on_val_seeds(val_pairs: List[Dict[str, Any]], transformer, consistency, cfg_te: TrainEvalRealtimeConfig) -> pd.DataFrame:
    rows = []
    for pair in val_pairs:
        seed = pair['seed']
        actual_env = pair['actual_env']
        pred_env = pair['pred_env']
        rt_plan = build_learned_realtime_corrected_plan(actual_env, pred_env, transformer, consistency, cfg_te, cfg_te.consistency_steps_eval)
        _, m_rt = execute_plan_on_env(actual_env, rt_plan, 'Learned_TC_RTCorrected', 'consistency', cfg_te.consistency_steps_eval, cfg_te.exp_cfg)
        m_rt['seed'] = seed
        rows.append(m_rt)
    return pd.DataFrame(rows)

# =========================================================
# Training (predicted history -> actual future target)
# =========================================================

def train_on_seed_pool(
    env_pairs: List[Dict[str, Any]],
    cfg_te: TrainEvalRealtimeConfig,
    val_pairs: List[Dict[str, Any]] | None = None,
):
    device = get_device()
    print(f"[train] start | device={device}")
    first = env_pairs[0]
    _, N = first["actual_env"].A_final.shape
    L, H = cfg_te.L, cfg_te.H
    transformer = OnlineTransformerPredictor(num_nodes=N, L=L, H=H).to(device)
    online_consistency = OnlineConsistencyGenerator(num_nodes=N, H=H).to(device)
    target_consistency = copy.deepcopy(online_consistency).to(device)
    target_consistency.eval()
    for p in target_consistency.parameters():
        p.requires_grad = False
    reward_weights_module = None
    if getattr(cfg_te, "reward_learnable", False):
        reward_weights_module = LearnableRewardWeights(cfg_te, device)
        optimizer = optim.Adam(list(transformer.parameters()) + list(online_consistency.parameters()) + list(reward_weights_module.parameters()), lr=cfg_te.lr)
    else:
        optimizer = optim.Adam(list(transformer.parameters()) + list(online_consistency.parameters()), lr=cfg_te.lr)
    bce_loss_fn = nn.BCELoss()
    history_rows = []
    t0 = time.time()
    best_val_score = -float("inf")

    for epoch in range(cfg_te.epochs):
        transformer.train(); online_consistency.train()
        epoch_loss = epoch_loss_tf = epoch_loss_rl = epoch_loss_cs = 0.0
        epoch_reward = 0.0
        epoch_pred_reward = 0.0
        step_count = 0

        for pair in env_pairs:
            actual_env = pair['actual_env']
            pred_env = pair['pred_env']
            A_in = np.asarray(pred_env.A_final, dtype=np.float32)
            A_target = np.asarray(actual_env.A_final, dtype=np.float32)
            U_target = np.asarray(actual_env.utility, dtype=np.float32)
            T, N2 = A_in.shape
            if N2 != N:
                raise ValueError(f'num_nodes mismatch in training: expected {N}, got {N2}')

            for t in range(L, T - H, cfg_te.stride):
                history_A = A_in[t-L:t, :]
                history_A = _augment_history_matrix(history_A, actual_env, t, cfg_te)
                future_A_true = A_target[t:t+H, :]
                state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)
                future_tensor_true = torch.tensor(future_A_true, dtype=torch.float32, device=device).unsqueeze(0)
                optimizer.zero_grad()
                future_pred = transformer(state_tensor)
                loss_tf = bce_loss_fn(future_pred, future_tensor_true)

                condition = future_pred.view(1, -1).detach()
                y_noisy_t2 = torch.randn(1, 2, device=device)
                y_noisy_t1 = y_noisy_t2 * 0.5
                y_curr = online_consistency(y_noisy_t1, condition, torch.tensor([[1.0]], device=device))

                current_node_approx = _best_available_node(A_target[t - 1], U_target[t - 1]) if t - 1 >= 0 else -1
                prev_node_approx = _best_available_node(A_target[t - 2], U_target[t - 2]) if t - 2 >= 0 else -1

                if reward_weights_module is not None:
                    w_u, w_o, w_s, w_r, w_j, w_p = reward_weights_module.to_numpy_weights()
                    best_i, best_n, reward_val = _find_best_reward_guided_target_with_weights(
                        future_A_true, U_target[t:t+H, :], current_node_approx, prev_node_approx,
                        w_u, w_o, w_s, w_r, w_j, w_p,
                        float(cfg_te.reward_gamma), float(getattr(cfg_te, 'reward_jitter_min_hold_ratio', 0.35)),
                        bool(getattr(cfg_te, 'reward_use_discounted_return', True)), float(cfg_te.reward_invalid_penalty),
                        time_offset_penalty=float(getattr(cfg_te, 'reward_time_offset_penalty', 0.0)),
                        tie_breaker_epsilon=float(getattr(cfg_te, 'reward_tie_breaker_epsilon', 1e-6)),
                    )
                    coefs = _reward_coefficients_for_action(future_A_true, U_target[t:t+H, :], best_i, best_n, current_node_approx, prev_node_approx, float(cfg_te.reward_gamma), float(getattr(cfg_te, 'reward_jitter_min_hold_ratio', 0.35)), bool(getattr(cfg_te, 'reward_use_discounted_return', True)))
                    coef_tensor = torch.tensor(coefs, dtype=torch.float32, device=device)
                    reward_torch = (coef_tensor * reward_weights_module()).sum() - (float(getattr(cfg_te, 'reward_time_offset_penalty', 0.0)) * best_i)
                    rl_weight_torch = 1.0 + cfg_te.reward_guided_target_scale * torch.tanh(reward_torch / 2.0)
                    rl_weight_torch = torch.clamp(rl_weight_torch, cfg_te.reward_guided_target_min_weight, cfg_te.reward_guided_target_max_weight)
                    reward_val = float(reward_torch.detach().item())
                else:
                    best_i, best_n, reward_val = _find_best_reward_guided_target(future_A_true, U_target[t:t+H, :], current_node_approx, cfg_te, prev_node_approx)
                    rl_weight_torch = _reward_to_supervision_weight(reward_val, cfg_te)

                epoch_reward += float(reward_val)
                y_star = torch.tensor([[0.0 if H <= 1 else best_i / float(H - 1), 0.0 if N <= 1 else best_n / float(N - 1)]], dtype=torch.float32, device=device)
                loss_oracle = rl_weight_torch * F.mse_loss(y_curr, y_star)
                loss_pred = _soft_expected_action_loss(y_curr, future_A_true, U_target[t:t+H, :], current_node_approx, prev_node_approx, cfg_te)
                loss_rollout = _mini_rollout_penalty(transformer, online_consistency, A_in, A_target, U_target, t, current_node_approx, prev_node_approx, cfg_te, device)
                loss_rl = float(getattr(cfg_te, 'alpha_oracle_loss', 0.6)) * loss_oracle + float(getattr(cfg_te, 'beta_pred_action_loss', 0.4)) * loss_pred + float(getattr(cfg_te, 'rollout_loss_weight', 0.35)) * loss_rollout

                pred_i = int(torch.clamp((y_curr[0, 0] * (H - 1)).round(), 0, H - 1).item())
                pred_n = int(torch.clamp((y_curr[0, 1] * (N - 1)).round(), 0, N - 1).item())
                epoch_pred_reward += _reward_for_action(future_A_true, U_target[t:t+H, :], pred_i, pred_n, current_node_approx, prev_node_approx, cfg_te)

                with torch.no_grad():
                    y_target = target_consistency(y_noisy_t2, condition, torch.tensor([[2.0]], device=device))
                loss_cs = F.mse_loss(y_curr, y_target)
                total_loss = loss_tf + loss_rl + (cfg_te.lambda_consistency * loss_cs)
                total_loss.backward()
                torch.nn.utils.clip_grad_norm_(list(transformer.parameters()) + list(online_consistency.parameters()), 1.0)
                optimizer.step()
                update_ema_target(online_consistency, target_consistency)
                epoch_loss += float(total_loss.item()); epoch_loss_tf += float(loss_tf.item()); epoch_loss_rl += float(loss_rl.item()); epoch_loss_cs += float(loss_cs.item()); step_count += 1

        row = {
            'epoch': epoch + 1,
            'steps': step_count,
            'avg_total_loss': epoch_loss / max(1, step_count),
            'avg_tf_loss': epoch_loss_tf / max(1, step_count),
            'avg_rl_loss': epoch_loss_rl / max(1, step_count),
            'avg_cs_loss': epoch_loss_cs / max(1, step_count),
            'avg_reward': epoch_reward / max(1, step_count),
            'avg_pred_reward': epoch_pred_reward / max(1, step_count),
            'elapsed_sec': time.time() - t0,
        }
        print(f"[train] epoch {epoch+1:02d}/{cfg_te.epochs} | loss={row['avg_total_loss']:.4f} (tf={row['avg_tf_loss']:.4f}, rl={row['avg_rl_loss']:.4f}, cs={row['avg_cs_loss']:.4f}) | oracle_reward={row['avg_reward']:.4f} | pred_reward={row['avg_pred_reward']:.4f} | elapsed={row['elapsed_sec']:.1f}s")

        if val_pairs:
            transformer.eval(); online_consistency.eval()
            with torch.no_grad():
                df_val = evaluate_on_val_seeds(val_pairs, transformer, online_consistency, cfg_te)
            if len(df_val) > 0:
                val_avail = float(df_val['Availability'].mean())
                val_qoe = float(df_val['EffectiveQoE'].mean())
                val_pp = float(df_val['PingPong_Ratio'].mean())
                val_hof = float(df_val['HO_Failure_Ratio'].mean())
                val_score = val_avail + 0.3 * val_qoe - 0.5 * val_pp - 0.5 * val_hof
                row['val_score'] = val_score
                print(f"[val] epoch {epoch+1:02d}/{cfg_te.epochs} | score={val_score:.4f} | Avail={val_avail:.4f} | QoE={val_qoe:.4f} | PP={val_pp:.4f} | HOF={val_hof:.4f}")
                if val_score > best_val_score:
                    best_val_score = val_score
                    save_weights(transformer, online_consistency, cfg_te.weight_path, reward_weights_module)
                    print(f"[best] saved best checkpoint: {cfg_te.weight_path}")
            transformer.train(); online_consistency.train()

        history_rows.append(row)

    transformer.eval(); online_consistency.eval()
    hist_df = pd.DataFrame(history_rows)
    hist_df.to_csv(Path(cfg_te.results_dir) / 'train_history_real_env.csv', index=False)
    print(f"[OK] train history saved: {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")
    if best_val_score == -float('inf'):
        save_weights(transformer, online_consistency, cfg_te.weight_path, reward_weights_module)
    return transformer, online_consistency, hist_df


# =========================================================
# Inference helpers (learned planner)
# =========================================================

def _predict_target_from_history(
    transformer,
    consistency,
    history_A: np.ndarray,
    H: int,
    N: int,
    device: torch.device,
    consistency_steps: int = 2,
) -> Tuple[int, int]:
    """
    history_A: (L, N) binary matrix
    return: (target_time_offset, target_node_idx)
    """
    state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)

    with torch.no_grad():
        future_pred = transformer(state_tensor)
        condition = future_pred.view(1, -1)

        y_curr = torch.randn(1, 2, device=device)

        # configurable few-step consistency
        steps = max(1, int(consistency_steps))
        for s in range(steps, 0, -1):
            step_tensor = torch.tensor([[float(s)]], dtype=torch.float32, device=device)
            y_curr = consistency(y_curr, condition, step_tensor)

        target_time_offset = int(y_curr[0, 0].item() * (H - 1))
        target_node_idx = int(y_curr[0, 1].item() * (N - 1))

        target_time_offset = min(max(target_time_offset, 0), H - 1)
        target_node_idx = min(max(target_node_idx, 0), N - 1)

    return target_time_offset, target_node_idx


def build_learned_offline_plan_from_pred_env(
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    predicted env만 보고 만든 learned plan (offline planning).
    Offline output is treated as a proposal/prior, with light guardrails
    so the plan is less brittle when executed online.
    """
    device = next(transformer.parameters()).device
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)
    T, N_env = A_pred.shape
    N_model = getattr(transformer, "num_nodes", N_env)
    if N_env > N_model:
        raise ValueError(
            f"Env has {N_env} nodes but checkpoint model expects {N_model}. "
            "Use weights trained with at least as many nodes as your env (e.g. larger max_sats)."
        )

    L, H = cfg_te.L, cfg_te.H
    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1
    last_switch_t = -10**9

    def _score(t_: int, n_: int) -> float:
        if n_ < 0 or n_ >= N_env or A_pred[t_, n_] != 1:
            return -1e9
        return float(U_pred[t_, n_])

    for t in range(T):
        if t < L:
            available = np.where(A_pred[t] == 1)[0]
            if len(available) > 0:
                current_node = int(available[np.argmax(U_pred[t, available])])
            else:
                current_node = -1
            planned_idx[t] = current_node
            continue

        history_A = A_pred[t - L : t, :]  # (L, N_env)
        if N_env < N_model:
            history_A = np.pad(history_A, ((0, 0), (0, N_model - N_env)), mode="constant", constant_values=0)
        target_time_offset, target_node_idx = _predict_target_from_history(
            transformer=transformer,
            consistency=consistency,
            history_A=history_A,
            H=H,
            N=N_model,
            device=device,
            consistency_steps=consistency_steps,
        )
        target_node_idx = min(max(target_node_idx, 0), N_env - 1)

        # target_time_offset <= k means "switch now/soon" instead of strict == 0
        k = int(getattr(cfg_te, "rt_switch_time_offset_max", 1))
        candidate = current_node
        if (target_time_offset <= k) and A_pred[t, target_node_idx] == 1:
            candidate = int(target_node_idx)

        # light dwell + hysteresis even for offline proposal
        if candidate != current_node and candidate != -1:
            can_switch = True
            if (
                (t - last_switch_t) < int(getattr(cfg_te, "rt_min_dwell", 3))
                and current_node != -1
                and A_pred[t, current_node] == 1
            ):
                can_switch = False
            if can_switch and current_node != -1 and A_pred[t, current_node] == 1:
                s_cur = _score(t, current_node)
                s_new = _score(t, candidate)
                ratio = float(getattr(cfg_te, "rt_hysteresis_ratio", 0.08))
                if not (s_new >= s_cur * (1.0 + ratio)):
                    can_switch = False
            if can_switch:
                current_node = candidate
                last_switch_t = t

        # if current is dead, fallback to best available predicted node
        if current_node != -1 and A_pred[t, current_node] == 0:
            avail = np.where(A_pred[t] == 1)[0]
            current_node = int(avail[np.argmax(U_pred[t, avail])]) if len(avail) > 0 else -1

        planned_idx[t] = current_node

    return planned_idx


def build_learned_realtime_corrected_plan(
    actual_env,
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    실시간 보정 버전 (receding-horizon, MPC 스타일):
    - 모델은 기본적으로 predicted history를 사용
    - 지나간 슬롯은 실제 관측(actual)로 덮어씀 (observed prefix correction)
    - 슬롯마다 node reliability EMA 갱신
    - 저신뢰/비가용 제안은 현재 실제 가용 후보로 fallback
    - (옵션) HO 안정화 가드레일: dwell + hysteresis + ping-pong 억제
      -> 과도한 HO/핑퐁으로 latency/HO attempts가 폭증하는 현상을 완화
    """
    device = next(transformer.parameters()).device

    A_actual = np.asarray(actual_env.A_final)
    U_actual = np.asarray(actual_env.utility)
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)

    T, N = A_actual.shape
    N_model = getattr(transformer, "num_nodes", N)
    if N > N_model:
        raise ValueError(
            f"Env has {N} nodes but checkpoint model expects {N_model}. "
            "Use weights trained with at least as many nodes as your env (e.g. larger max_sats)."
        )
    L, H = cfg_te.L, cfg_te.H

    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1

    # 과거 관측 누적 반영용 (처음엔 predicted로 시작, 지나간 슬롯은 actual로 덮어씀)
    A_obs_corrected = A_pred.copy()
    # node 신뢰도 (0~1)
    reliability = np.ones(N, dtype=np.float32)

    # 마지막 스위치 시각(슬롯 인덱스)
    last_switch_t = -10**9

    # 디버그 카운터 (online TC 동작 분석용)
    dbg_total_slots = T
    dbg_model_proposed = 0              # 모델이 target_time_offset==0으로 switch 제안한 슬롯 수
    dbg_model_proposed_valid = 0        # 그 중 실제 가용 슬롯인 경우
    dbg_model_applied = 0               # 최종 플랜에서 모델 제안이 그대로 반영된 슬롯 수
    dbg_fallback_used = 0              # actual-slot greedy fallback이 사용된 슬롯 수
    dbg_guardrail_block = 0            # guardrail 때문에 switch가 막힌 횟수

    def _score(t_: int, n_: int) -> float:
        # utility * reliability (둘 다 0~대략 1 scale 기대)
        return float(U_actual[t_, n_] * reliability[n_])

    for t in range(T):
        # 현재 슬롯까지는 실제 관측값 사용 가능하다고 가정(실시간 측정)
        A_obs_corrected[t] = A_actual[t]

        prev1 = int(planned_idx[t - 1]) if t - 1 >= 0 else -1
        prev2 = int(planned_idx[t - 2]) if t - 2 >= 0 else -1

        # 현재 연결의 유효성(현재 슬롯에서 커버리지/정책상 가능?)
        current_valid_now = (current_node >= 0 and current_node < N and A_actual[t, current_node] == 1)

        # ------------------------ (A) 기본 제안 산출 ------------------------
        if t < L:
            # warmup: 현재 실제 가용 후보 중 신뢰도 반영 점수 최대 선택
            avail = np.where(A_actual[t] == 1)[0]
            if len(avail) > 0:
                score = U_actual[t, avail] * reliability[avail]
                proposed = int(avail[np.argmax(score)])
            else:
                proposed = -1

            corrected = proposed

        else:
            # history는 "과거 actual 관측 반영 + (미래는 여전히 predicted)"
            history_A = A_obs_corrected[t - L : t, :]  # (L, N)
            if N < N_model:
                history_A = np.pad(history_A, ((0, 0), (0, N_model - N)), mode="constant", constant_values=0)
            target_time_offset, target_node_idx = _predict_target_from_history(
                transformer=transformer,
                consistency=consistency,
                history_A=history_A,
                H=H,
                N=N_model,
                device=device,
                consistency_steps=consistency_steps,
            )
            target_node_idx = min(max(target_node_idx, 0), N - 1)

            k = int(getattr(cfg_te, "rt_switch_time_offset_max", 1))
            proposed = current_node
            if target_time_offset <= k:
                dbg_model_proposed += 1
                proposed = int(target_node_idx)

            # 실시간 보정 로직
            proposed_valid_now = (
                proposed is not None
                and proposed >= 0
                and proposed < N
                and A_actual[t, proposed] == 1
            )

            if proposed_valid_now:
                dbg_model_proposed_valid += 1

            low_rel = False
            if proposed is not None and proposed >= 0 and proposed < N:
                low_rel = (float(reliability[proposed]) < cfg_te.rt_low_reliability_threshold)

            corrected = current_node

            # (변경) 모델이 현재 슬롯에서 유효한 노드를 제안하면 신뢰도와 무관하게 일단 수용
            # (guardrail에서 추가 필터링)
            model_based_switch = False
            if proposed_valid_now:
                corrected = proposed
                model_based_switch = True
            else:
                # proposed가 지금 안되거나 (가용 X) 하면 fallback
                if cfg_te.rt_use_actual_current_slot_fallback:
                    avail = np.where(A_actual[t] == 1)[0]
                    if len(avail) > 0:
                        # 신뢰도 반영 + 현재 actual utility 기반 즉시 보정
                        score = U_actual[t, avail] * reliability[avail]
                        fallback = int(avail[np.argmax(score)])

                        # 현재 연결 유지가 유효하고 제안이 low_rel이면 유지를 우선할 수도 있음
                        if current_valid_now and low_rel:
                            corrected = current_node
                        else:
                            corrected = fallback
                            if corrected != current_node:
                                dbg_fallback_used += 1
                    else:
                        corrected = -1 if not current_valid_now else current_node
                else:
                    corrected = current_node if current_valid_now else -1

        # ------------------------ (B) HO 안정화 가드레일 ------------------------
        if cfg_te.rt_enable_guardrails:
            # 최종 후보 유효성
            corrected_valid_now = (corrected is not None and corrected >= 0 and corrected < N and A_actual[t, corrected] == 1)

            # 스위치 시도인지?
            switching = (corrected_valid_now and current_valid_now and corrected != current_node)

            if switching:
                blocked = False
                # (1) Dwell: 최근 스위치 후 일정 시간은 유지
                if (t - last_switch_t) < int(cfg_te.rt_min_dwell):
                    corrected = current_node
                    blocked = True
                else:
                    # (2) Hysteresis: 충분한 이득이 있을 때만 스위치
                    s_cur = _score(t, current_node)
                    s_new = _score(t, corrected)

                    required_ratio = float(cfg_te.rt_hysteresis_ratio)

                    # (3) Ping-pong(A->B->A) 억제: t-2 노드로 되돌아가려 하면 더 큰 히스테리시스 요구
                    is_pingpong = (
                        corrected == prev2
                        and prev2 != -1
                        and prev1 == current_node
                        and (t - last_switch_t) <= int(cfg_te.rt_pingpong_window)
                    )
                    if is_pingpong:
                        required_ratio += float(cfg_te.rt_pingpong_extra_hysteresis_ratio)

                    ratio_ok = (s_new >= s_cur * (1.0 + required_ratio))
                    if float(cfg_te.rt_hysteresis_abs) > 0.0:
                        abs_ok = ((s_new - s_cur) >= float(cfg_te.rt_hysteresis_abs))
                        ok = ratio_ok and abs_ok
                    else:
                        ok = ratio_ok

                    if not ok:
                        corrected = current_node
                        blocked = True

                if blocked:
                    dbg_guardrail_block += 1

            # 현재 연결이 유효한데 corrected가 -1이면 끊김 방지로 유지(단, 실제로 유효할 때만)
            if corrected == -1 and current_valid_now:
                corrected = current_node

        # ------------------------ (C) 최종 적용 & 스위치 기록 ------------------------
        # 최종 유효성 재검사
        if corrected != -1 and (corrected < 0 or corrected >= N or A_actual[t, corrected] != 1):
            corrected = -1

        # 스위치 발생 시각 기록 (연결이 유효한 상태에서 다른 유효 노드로 바뀐 경우)
        if corrected != current_node and corrected != -1:
            last_switch_t = t

        # 디버그: 최종적으로 모델 제안이 실제로 반영된 슬롯 카운트
        if t >= L and model_based_switch and corrected == proposed and corrected != current_node:
            dbg_model_applied += 1

        current_node = int(corrected) if corrected is not None else -1
        planned_idx[t] = current_node

        # ---- reliability update (현재 슬롯 관측으로 다음 슬롯 planning 보정에 반영) ----
        match = (A_pred[t].astype(np.int32) == A_actual[t].astype(np.int32)).astype(np.float32)
        reliability = cfg_te.rt_reliability_alpha * reliability + (1.0 - cfg_te.rt_reliability_alpha) * match

        # false positive 예측(예측=1, 실제=0)에는 추가 페널티
        fp_mask = (A_pred[t] == 1) & (A_actual[t] == 0)
        reliability[fp_mask] *= cfg_te.rt_fp_extra_penalty

        reliability = np.clip(reliability, 0.05, 1.0)

    # 디버그 로그 출력 (옵션)
    if getattr(cfg_te, "rt_debug_log", False):
        total_effective = max(1, dbg_total_slots - cfg_te.L)
        print("[RT-TC debug]")
        print(f"  total_slots                = {dbg_total_slots}")
        print(f"  model_proposed_slots       = {dbg_model_proposed}")
        print(f"  model_proposed_valid_slots = {dbg_model_proposed_valid}")
        print(f"  model_applied_switches     = {dbg_model_applied}")
        print(f"  fallback_used_switches     = {dbg_fallback_used}")
        print(f"  guardrail_blocked_switches = {dbg_guardrail_block}")
        print(f"  model_applied_ratio        = {dbg_model_applied / float(total_effective):.4f}")

    return planned_idx


# =========================================================
# Evaluation on hold-out seeds
# =========================================================

def evaluate_on_test_seeds(
    test_pairs: List[Dict[str, Any]],
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    rows = []
    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    print("\n=== Evaluation: Learned planner on held-out seeds ===")
    for pair in test_pairs:
        seed = pair["seed"]
        actual_env = pair["actual_env"]
        pred_env = pair["pred_env"]

        np.random.seed(seed)  # runtime miss stochasticity 재현성 확보

        # 1) Baseline: sustainable DAG on actual env
        dag_plan = sustainable_dag_plan(actual_env, cfg_te.exp_cfg)
        tl_dag, m_dag = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=dag_plan,
            method_name="Sustainable_DAG_NetworkX",
            mode="sustainable_dag",
            sampling_steps=0,
            cfg=cfg_te.exp_cfg,
        )
        m_dag["seed"] = seed
        rows.append(m_dag)

        # 2) Learned offline (predicted env only)
        offline_plan = build_learned_offline_plan_from_pred_env(
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_off, m_off = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=offline_plan,
            method_name="Learned_TC_Offline",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_off["seed"] = seed
        rows.append(m_off)

        # 3) Learned + realtime correction
        rt_plan = build_learned_realtime_corrected_plan(
            actual_env=actual_env,
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_rt, m_rt = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=rt_plan,
            method_name="Learned_TC_RTCorrected",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_rt["seed"] = seed
        rows.append(m_rt)

        print(
            f"[seed={seed}]\n"
            f" 🔹 [DAG] Avail: {m_dag['Availability']:.4f} | QoE: {m_dag['EffectiveQoE']:.4f} | 핑퐁: {m_dag['PingPong_Ratio']:.4f} | HO실패: {m_dag['HO_Failure_Ratio']:.4f}\n"
            f" 🔸 [ AI] Avail: {m_rt['Availability']:.4f} | QoE: {m_rt['EffectiveQoE']:.4f} | 핑퐁: {m_rt['PingPong_Ratio']:.4f} | HO실패: {m_rt['HO_Failure_Ratio']:.4f}\n"
            f" --------------------------------------------------------"
        )

        # seed0 timeline 저장 (test 첫 seed)
        if seed == test_pairs[0]["seed"]:
            tl_dag.to_csv(results_dir / f"timeline_sustainable_dag_seed{seed}.csv", index=False)
            tl_off.to_csv(results_dir / f"timeline_learned_tc_offline_seed{seed}.csv", index=False)
            tl_rt.to_csv(results_dir / f"timeline_learned_tc_rtcorrected_seed{seed}.csv", index=False)

    df_runs = pd.DataFrame(rows)
    df_summary = summarize_runs(df_runs)

    df_runs.to_csv(results_dir / "exp3_learned_realtime_runs.csv", index=False)
    df_summary.to_csv(results_dir / "exp3_learned_realtime_summary.csv", index=False)

    print(f"✅ saved: {results_dir / 'exp3_learned_realtime_runs.csv'}")
    print(f"✅ saved: {results_dir / 'exp3_learned_realtime_summary.csv'}")
    return df_runs, df_summary


# =========================================================
# Main pipeline
# =========================================================

def main():
    cfg_te = TrainEvalRealtimeConfig()

    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    # experiments_atg_leo의 env builder가 쓰는 TLE 파일 확인
    if not Path(cfg_te.exp_cfg.tle_path).exists():
        raise FileNotFoundError(
            f"Frozen TLE file not found: {cfg_te.exp_cfg.tle_path}\n"
            "먼저 TLE 파일 경로를 맞춰주세요."
        )

    # split 저장
    split_manifest = {
        "train_seeds": cfg_te.train_seeds,
        "val_seeds": cfg_te.val_seeds,
        "test_seeds": cfg_te.test_seeds,
    }
    with open(cfg_te.split_manifest_path, "w", encoding="utf-8") as f:
        json.dump(split_manifest, f, ensure_ascii=False, indent=2)

    print("=== Seed Split ===")
    print(json.dumps(split_manifest, ensure_ascii=False, indent=2))

    t0 = time.time()

    # 1) collect
    train_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.train_seeds)
    val_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.val_seeds)   # 지금은 로깅용/확장용
    test_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.test_seeds)

    print(f"[collect done] train={len(train_pairs)}, val={len(val_pairs)}, test={len(test_pairs)}")

    # 2) train
    transformer, consistency, train_hist = train_on_seed_pool(train_pairs, cfg_te, val_pairs=val_pairs)

    # (선택) val quick sanity metrics를 원하면 여기 추가 가능
    # 지금은 바로 test 평가

    # 3) evaluate (hold-out)
    df_runs, df_summary = evaluate_on_test_seeds(
        test_pairs=test_pairs,
        transformer=transformer,
        consistency=consistency,
        cfg_te=cfg_te,
    )

    elapsed = time.time() - t0
    print("\n=== Done ===")
    print(f"Total elapsed: {elapsed:.2f} sec")
    print("Outputs:")
    print(f" - {cfg_te.weight_path}")
    print(f" - {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_runs.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_summary.csv'}")
    print(f" - {cfg_te.split_manifest_path}")

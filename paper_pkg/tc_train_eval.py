#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
paper_pkg.tc_train_eval (CLEAN)

Core training/evaluation module for Transformer + Consistency (TC), with realtime guardrails.

This is the import-safe version:
- NO CLI main()
- Imports env/execution from paper_pkg.env_core
- Used by paper_pkg.train and paper_pkg.eval
"""

from __future__ import annotations

from dataclasses import dataclass, field
from pathlib import Path
from types import SimpleNamespace
from typing import List, Tuple, Dict, Any
import copy
import json
import time
import math

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# === ê¸°ì¡´ ì‹¤í—˜ íŒŒì¼ì—ì„œ ì¬ì‚¬ìš© ===
from .env_core import (
    ExperimentConfig,
    build_env,
    execute_plan_on_env,
    sustainable_dag_plan,
    summarize_runs,
)

# === ëª¨ë¸ import (í”„ë¡œì íŠ¸ êµ¬ì¡°ì— ë”°ë¼ ë‘˜ ì¤‘ í•˜ë‚˜ê°€ ë§ì„ ìˆ˜ ìˆìŒ) ===
try:
    from models.proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator
except ImportError:
    from proposed_tc_planner import OnlineTransformerPredictor, OnlineConsistencyGenerator


# =========================================================
# Config
# =========================================================

@dataclass
class TrainEvalRealtimeConfig:
    exp_cfg: ExperimentConfig = field(default_factory=ExperimentConfig)

    # ë°ì´í„° ë¶„í•  (ì˜ˆì‹œ)
    train_seeds: List[int] = field(default_factory=lambda: [0])
    val_seeds: List[int] = field(default_factory=lambda: [1])
    test_seeds: List[int] = field(default_factory=lambda: [2])

    # ëª¨ë¸/í•™ìŠµ
    L: int = 20
    H: int = 30
    epochs: int = 3
    stride: int = 5
    lr: float = 1e-3
    lambda_consistency: float = 1.0
    reward_invalid_penalty: float = -1.0

    # ---- í†µí•© ë³´ìƒ(mini) í•˜ì´í¼íŒŒë¼ë¯¸í„° ----
    # (ë‹¨ì¼ í•­ê³µê¸° ì‹œë‚˜ë¦¬ì˜¤ì´ë¯€ë¡œ LBëŠ” ì œì™¸)
    reward_gamma: float = 0.97                 # í• ì¸ê³„ìˆ˜ Î³
    reward_w_utility: float = 1.00             # ê°€ìš© ìŠ¬ë¡¯ utility ë³´ìƒ
    reward_w_outage: float = 0.70              # ë¹„ê°€ìš©(outage) íŒ¨ë„í‹°
    reward_w_switch: float = 0.05              # í•¸ë“œì˜¤ë²„(ìŠ¤ìœ„ì¹˜) íŒ¨ë„í‹°
    reward_w_robustness: float = 0.20          # í›„ë³´ ë…¸ë“œì˜ ë¯¸ë˜ ê°€ìš©ì„±(robustness) ë³´ë„ˆìŠ¤
    reward_w_jitter: float = 0.15              # ì§§ì€ ìœ ì§€ì‹œê°„ ê¸°ë°˜ ì§€í„° proxy íŒ¨ë„í‹°
    reward_w_pingpong: float = 0.20            # A->B->A í˜•íƒœ ping-pong proxy íŒ¨ë„í‹°
    reward_jitter_min_hold_ratio: float = 0.35 # ì´ ë¹„ìœ¨ë³´ë‹¤ ì§§ê²Œ ìœ ì§€ë˜ë©´ ì§€í„° íŒ¨ë„í‹° ê°•í™”
    reward_use_discounted_return: bool = True  # í• ì¸ ëˆ„ì ë³´ìƒ ì‚¬ìš© ì—¬ë¶€
    reward_guided_target_scale: float = 1.0    # RL-loss ê°€ì¤‘ ìŠ¤ì¼€ì¼ (ë³´ìƒ ê¸°ë°˜)
    reward_guided_target_min_weight: float = 0.1
    reward_guided_target_max_weight: float = 3.0

    # ì„¤ê³„ì•ˆ B: "ì¼ì° ìŠ¤ìœ„ì¹˜" ìœ ë„ â€” iê°€ í´ìˆ˜ë¡ ë³´ìƒì—ì„œ ì¶”ê°€ íŒ¨ë„í‹°
    reward_time_offset_penalty: float = 0.02   # total -= penalty * i (i=0 ì„ í˜¸)
    reward_tie_breaker_epsilon: float = 1e-6    # ë³´ìƒ ì°¨ì´ < epsë©´ ê°™ì€ ê²ƒìœ¼ë¡œ ë³´ê³  ë” ì‘ì€ i ì„ í˜¸

    # ì¶”ë¡ : target_time_offset <= k ì´ë©´ "ì§€ê¸ˆ/ê³§ ìŠ¤ìœ„ì¹˜"ë¡œ í•´ì„ (ê¸°ë³¸ 1 = 0 ë˜ëŠ” 1 ìŠ¬ë¡¯ ì•ˆ)
    rt_switch_time_offset_max: int = 1

    # í•™ìŠµ ì¤‘ ë³´ìƒ ê°€ì¤‘ì¹˜ í•™ìŠµ ì—¬ë¶€ (Trueë©´ w_utility, w_outage ë“±ì´ nn.Parameterë¡œ ìµœì í™”)
    reward_learnable: bool = False

    # í‰ê°€ ì‹œ consistency sampling steps
    consistency_steps_eval: int = 2

    # ì‹¤ì‹œê°„ ë³´ì • (online correction)
    rt_reliability_alpha: float = 0.90  # EMA: í´ìˆ˜ë¡ ëŠë¦¬ê²Œ ë³€í•¨
    rt_low_reliability_threshold: float = 0.45
    rt_fp_extra_penalty: float = 0.90    # false positive(ì˜ˆì¸¡=1 ì‹¤ì œ=0) ì¶”ê°€ ê°ì‡ 
    rt_use_actual_current_slot_fallback: bool = True


    # ---- ì‹¤ì‹œê°„ HO ì•ˆì •í™” ê°€ë“œë ˆì¼(í•‘í/ê³¼ë„í•œ HO ì–µì œ) ----
    # enable_guardrails=Falseë¡œ ë‘ë©´ ê¸°ì¡´ ë™ì‘(ë§¤ ìŠ¬ë¡¯ ì¦‰ì‹œ ë³´ì •/ìŠ¤ìœ„ì¹˜)ì„ ê·¸ëŒ€ë¡œ ìœ ì§€
    rt_enable_guardrails: bool = True

    # (1) ìµœì†Œ ìœ ì§€ì‹œê°„: ìµœê·¼ ìŠ¤ìœ„ì¹˜ í›„ rt_min_dwell ìŠ¬ë¡¯ ë™ì•ˆì€ ìŠ¤ìœ„ì¹˜ ê¸ˆì§€(í˜„ì¬ ì—°ê²°ì´ ìœ íš¨í•œ ê²½ìš°)
    rt_min_dwell: int = 3

    # (2) íˆìŠ¤í…Œë¦¬ì‹œìŠ¤: ìŠ¤ìœ„ì¹˜í•˜ë ¤ë©´ í˜„ì¬ ëŒ€ë¹„ ì¶©ë¶„í•œ ì´ë“ì´ ìˆì–´ì•¼ í•¨
    # - ratio ê¸°ì¤€: score_new >= score_cur * (1 + rt_hysteresis_ratio)
    # - abs ê¸°ì¤€: (ì„ íƒ) score_new - score_cur >= rt_hysteresis_abs
    rt_hysteresis_ratio: float = 0.08
    rt_hysteresis_abs: float = 0.0

    # (3) í•‘í(A->B->A) ì–µì œ: ìµœê·¼ rt_pingpong_window ìŠ¬ë¡¯ ë‚´ ë˜ëŒë¦¼ì´ë©´ ë” í° íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ìš”êµ¬
    rt_pingpong_window: int = 4
    rt_pingpong_extra_hysteresis_ratio: float = 0.12

    # ---- Realtime planning enhancements ----
    rt_enable_pending: bool = True
    rt_fallback_mode: str = "lookahead"  # myopic | lookahead
    rt_fallback_alpha_latency: float = 0.05
    rt_fallback_gamma: float = 0.97
    rt_fallback_H: int = 30
    rt_debug_log: bool = False

    # ì €ì¥
    results_dir: str = "results"
    weight_path: str = "results/trained_weights_rl_real_env.pth"
    split_manifest_path: str = "results/train_val_test_seed_split.json"


# =========================================================
# Utilities
# =========================================================

def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    if torch.backends.mps.is_available():
        return torch.device("mps")
    return torch.device("cpu")


def update_ema_target(online_net, target_net, tau=0.05):
    with torch.no_grad():
        for online_param, target_param in zip(online_net.parameters(), target_net.parameters()):
            target_param.data.copy_(tau * online_param.data + (1.0 - tau) * target_param.data)


def save_weights(transformer, consistency, weight_path: str, reward_weights_module=None):
    Path(weight_path).parent.mkdir(parents=True, exist_ok=True)
    payload = {
        "transformer_state_dict": transformer.state_dict(),
        "consistency_state_dict": consistency.state_dict(),
        "num_nodes": getattr(transformer, "num_nodes", None),
        "L": getattr(transformer, "L", None),
        "H": getattr(transformer, "H", None),
    }
    if reward_weights_module is not None:
        payload["reward_weights_state_dict"] = reward_weights_module.state_dict()
    torch.save(payload, weight_path)
    print(f"âœ… weights saved: {weight_path}")


def load_weights(transformer, consistency, weight_path: str, device: torch.device):
    ckpt = torch.load(weight_path, map_location=device)
    transformer.load_state_dict(ckpt["transformer_state_dict"])
    consistency.load_state_dict(ckpt["consistency_state_dict"])


# =========================================================
# Seed collection (actual / predicted env pair)
# =========================================================

def collect_env_pairs(cfg: ExperimentConfig, seeds: List[int]) -> List[Dict[str, Any]]:
    pairs = []
    for seed in seeds:
        print(f"[collect] seed={seed}")
        actual_env = build_env(cfg, seed=seed, predicted_view=False)
        pred_env = build_env(cfg, seed=seed, predicted_view=True)

        # ê¸°ë³¸ shape ì²´í¬
        if actual_env.A_final.shape != pred_env.A_final.shape:
            raise ValueError(f"shape mismatch at seed={seed}: actual {actual_env.A_final.shape} vs pred {pred_env.A_final.shape}")
        if actual_env.A_final.shape[1] != pred_env.A_final.shape[1]:
            raise ValueError(f"N mismatch at seed={seed}")

        pairs.append(
            {
                "seed": seed,
                "actual_env": actual_env,
                "pred_env": pred_env,
            }
        )
    return pairs



# =========================================================
# Learnable reward weights (optional)
# =========================================================

def _inverse_softplus(y: float) -> float:
    """softplus(x)=y => x = log(exp(y)-1). y>0."""
    if y <= 0:
        return -2.0
    return float(math.log(math.exp(y) - 1.0))


class LearnableRewardWeights(nn.Module):
    """ë³´ìƒ ê°€ì¤‘ì¹˜ 6ê°œë¥¼ nn.Parameterë¡œ ë‘ê³  softplusë¡œ ì–‘ìˆ˜ ìœ ì§€."""
    def __init__(self, cfg_te: TrainEvalRealtimeConfig, device: torch.device):
        super().__init__()
        defaults = (
            cfg_te.reward_w_utility,
            getattr(cfg_te, "reward_w_outage", 0.7),
            getattr(cfg_te, "reward_w_switch", 0.05),
            getattr(cfg_te, "reward_w_robustness", 0.2),
            getattr(cfg_te, "reward_w_jitter", 0.15),
            getattr(cfg_te, "reward_w_pingpong", 0.2),
        )
        init_vals = [_inverse_softplus(float(d)) for d in defaults]
        self.log_weights = nn.Parameter(torch.tensor(init_vals, dtype=torch.float32, device=device))

    def forward(self) -> torch.Tensor:
        return F.softplus(self.log_weights)  # (6,) w_u, w_o, w_s, w_r, w_j, w_p

    def to_numpy_weights(self) -> Tuple[float, float, float, float, float, float]:
        w = self.forward().detach().cpu().numpy()
        return (float(w[0]), float(w[1]), float(w[2]), float(w[3]), float(w[4]), float(w[5]))


# =========================================================
# Reward helpers (mini integrated reward for single-aircraft)
# =========================================================

def _best_available_node(A_row: np.ndarray, U_row: np.ndarray) -> int:
    avail = np.where(A_row == 1)[0]
    if len(avail) == 0:
        return -1
    return int(avail[np.argmax(U_row[avail])])


def _contiguous_available_run_len(future_A_true: np.ndarray, start_i: int, node_idx: int) -> int:
    """start_ië¶€í„° ì—°ì† ê°€ìš©(1)ì¸ ê¸¸ì´. ì²« ìŠ¬ë¡¯ì´ 0ì´ë©´ 0."""
    H, _ = future_A_true.shape
    run_len = 0
    for tau in range(int(start_i), H):
        if int(future_A_true[tau, node_idx]) != 1:
            break
        run_len += 1
    return int(run_len)


def _discounted_integrated_reward_single_aircraft(
    future_A_true: np.ndarray,   # (H, N)
    future_U_true: np.ndarray,   # (H, N)
    target_time_offset: int,
    target_node_idx: int,
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> float:
    """
    ë…¼ë¬¸ì‹ í†µí•© ë³´ìƒì˜ 'ë¯¸ë‹ˆ ë²„ì „' (ë‹¨ì¼ í•­ê³µê¸°):
      - utility í•­ (ì •ê·œí™”ëœ utility ì‚¬ìš©)
      - outage íŒ¨ë„í‹°
      - switch(í•¸ë“œì˜¤ë²„) ë¹„ìš©
      - robustness ë³´ë„ˆìŠ¤(ì„ íƒ ë…¸ë“œì˜ ë¯¸ë˜ ê°€ìš© ë¹„ìœ¨)
      - jitter proxy íŒ¨ë„í‹° (ì§§ì€ ìœ ì§€ì‹œê°„ì— ìŠ¤ìœ„ì¹˜í• ìˆ˜ë¡ ë¶ˆë¦¬)
      - ping-pong proxy íŒ¨ë„í‹° (A->B->A)
    â€» Load Balance(LB)ëŠ” ë‹¨ì¼ í•­ê³µê¸°ë¼ ì œì™¸
    """
    H, N = future_A_true.shape
    i = int(np.clip(target_time_offset, 0, H - 1))
    n = int(np.clip(target_node_idx, 0, N - 1))

    gamma = float(cfg_te.reward_gamma)
    w_u = float(cfg_te.reward_w_utility)
    w_o = float(cfg_te.reward_w_outage)
    w_s = float(cfg_te.reward_w_switch)
    w_r = float(cfg_te.reward_w_robustness)
    w_j = float(getattr(cfg_te, "reward_w_jitter", 0.0))
    w_p = float(getattr(cfg_te, "reward_w_pingpong", 0.0))
    jitter_thr = float(getattr(cfg_te, "reward_jitter_min_hold_ratio", 0.35))

    if i >= H:
        return float(cfg_te.reward_invalid_penalty)

    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    total = 0.0

    # ì„ íƒ ì‹œì ì— 1íšŒ ìŠ¤ìœ„ì¹˜ ë¹„ìš©
    switch_happens = bool(current_node != -1 and current_node != n)
    if switch_happens:
        total -= w_s

        # ---- Jitter proxy: ìŠ¤ìœ„ì¹˜í–ˆëŠ”ë° ê³§ë°”ë¡œ ëŠê¸¸ìˆ˜ë¡ ë¶ˆë¦¬ ----
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)  # 0~1
        # threshold ì´í•˜ë©´ ì„ í˜• íŒ¨ë„í‹°, ì´ìƒì´ë©´ 0
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        total -= (w_j * jitter_pen)

        # ---- Ping-pong proxy: ì§ì „ ë…¸ë“œ(prev_node)ë¡œ ë˜ëŒì•„ê°€ëŠ” A->B->A íŒ¨í„´ ----
        # prev_node --(ì´ì „ ìŠ¬ë¡¯)--> current_node --(ì´ë²ˆ ê²°ì •)--> n
        if prev_node != -1 and current_node != -1 and prev_node == n and current_node != n:
            total -= w_p

    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        inst = (w_u * u * a) - (w_o * (1.0 - a))
        if tau == i:
            inst += (w_r * robustness)
        if cfg_te.reward_use_discounted_return:
            total += (gamma ** (tau - i)) * inst
        else:
            total += inst

    # ì„¤ê³„ì•ˆ B: iê°€ í´ìˆ˜ë¡ íŒ¨ë„í‹° (ì¼ì° ìŠ¤ìœ„ì¹˜ ìœ ë„)
    total -= float(getattr(cfg_te, "reward_time_offset_penalty", 0.0)) * i
    return float(total)


def _discounted_integrated_reward_with_weights(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    target_time_offset: int,
    target_node_idx: int,
    current_node: int,
    prev_node: int,
    w_u: float, w_o: float, w_s: float, w_r: float, w_j: float, w_p: float,
    gamma: float, jitter_thr: float, use_discounted: bool,
    time_offset_penalty: float = 0.0,
) -> float:
    """ë³´ìƒ ê³„ì‚° (ê°€ì¤‘ì¹˜ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…). í•™ìŠµ ê°€ëŠ¥ ê°€ì¤‘ì¹˜ìš©."""
    H, N = future_A_true.shape
    i = int(np.clip(target_time_offset, 0, H - 1))
    n = int(np.clip(target_node_idx, 0, N - 1))
    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    total = 0.0
    switch_happens = bool(current_node != -1 and current_node != n)
    if switch_happens:
        total -= w_s
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        total -= w_j * jitter_pen
        if prev_node != -1 and current_node != -1 and prev_node == n and current_node != n:
            total -= w_p
    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        inst = (w_u * u * a) - (w_o * (1.0 - a))
        if tau == i:
            inst += w_r * robustness
        total += (gamma ** (tau - i)) * inst if use_discounted else inst
    total -= time_offset_penalty * i
    return float(total)


def _find_best_reward_guided_target(
    future_A_true: np.ndarray,    # (H, N)
    future_U_true: np.ndarray,    # (H, N)
    current_node: int,
    cfg_te: TrainEvalRealtimeConfig,
    prev_node: int = -1,
) -> Tuple[int, int, float]:
    """
    ë¯¸ë˜ H-step í›„ë³´ (time_offset, node)ë¥¼ ìŠ¤ìº”í•´ì„œ í†µí•©ë³´ìƒ(mini) ê¸°ì¤€ ìµœì  í›„ë³´ë¥¼ ì„ íƒ.
    ìœ íš¨ í›„ë³´ê°€ ì—†ìœ¼ë©´ (0,0, invalid_penalty) ë°˜í™˜.
    """
    H, N = future_A_true.shape
    best_i, best_n = 0, 0
    best_r = -1e18

    any_valid = False
    for i in range(H):
        valid_nodes = np.where(future_A_true[i] == 1)[0]
        if len(valid_nodes) == 0:
            continue
        any_valid = True
        for n in valid_nodes:
            r = _discounted_integrated_reward_single_aircraft(
                future_A_true=future_A_true,
                future_U_true=future_U_true,
                target_time_offset=i,
                target_node_idx=int(n),
                current_node=current_node,
                cfg_te=cfg_te,
            )
            eps = float(getattr(cfg_te, "reward_tie_breaker_epsilon", 1e-6))
            if r > best_r or (abs(r - best_r) < eps and i < best_i):
                best_r = float(r)
                best_i, best_n = int(i), int(n)

    if not any_valid:
        return 0, 0, float(cfg_te.reward_invalid_penalty)

    return best_i, best_n, float(best_r)


def _find_best_reward_guided_target_with_weights(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    current_node: int,
    prev_node: int,
    w_u: float, w_o: float, w_s: float, w_r: float, w_j: float, w_p: float,
    gamma: float, jitter_thr: float, use_discounted: bool,
    invalid_penalty: float,
    time_offset_penalty: float = 0.0,
    tie_breaker_epsilon: float = 1e-6,
) -> Tuple[int, int, float]:
    """ë³´ìƒ ê°€ì¤‘ì¹˜ë¥¼ ì™¸ë¶€ì—ì„œ ì£¼ì…í•˜ì—¬ best (i, n) ë° ë³´ìƒê°’ ë°˜í™˜. í•™ìŠµ ê°€ëŠ¥ ê°€ì¤‘ì¹˜ìš©."""
    H, N = future_A_true.shape
    best_i, best_n = 0, 0
    best_r = -1e18
    any_valid = False
    for i in range(H):
        valid_nodes = np.where(future_A_true[i] == 1)[0]
        if len(valid_nodes) == 0:
            continue
        any_valid = True
        for n in valid_nodes:
            r = _discounted_integrated_reward_with_weights(
                future_A_true, future_U_true, i, int(n), current_node, prev_node,
                w_u, w_o, w_s, w_r, w_j, w_p, gamma, jitter_thr, use_discounted,
                time_offset_penalty=time_offset_penalty,
            )
            if r > best_r or (abs(r - best_r) < tie_breaker_epsilon and i < best_i):
                best_r = r
                best_i, best_n = i, int(n)
    if not any_valid:
        return 0, 0, invalid_penalty
    return best_i, best_n, float(best_r)


def _reward_coefficients_for_action(
    future_A_true: np.ndarray,
    future_U_true: np.ndarray,
    best_i: int,
    best_n: int,
    current_node: int,
    prev_node: int,
    gamma: float,
    jitter_thr: float,
    use_discounted: bool,
) -> Tuple[float, float, float, float, float, float]:
    """
    R = c_u*w_u + c_o*w_o + c_s*w_s + c_r*w_r + c_j*w_j + c_p*w_p ì¸ ê³„ìˆ˜ (c_u,...,c_p) ë°˜í™˜.
    í•™ìŠµ ê°€ëŠ¥ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ gradientë¥¼ ìœ„í•´ ì‚¬ìš©.
    """
    H, N = future_A_true.shape
    i = int(np.clip(best_i, 0, H - 1))
    n = int(np.clip(best_n, 0, N - 1))
    robustness = float(np.mean(future_A_true[i:, n])) if i < H else 0.0
    switch_happens = bool(current_node != -1 and current_node != n)
    # utility ê³„ìˆ˜: sum_tau gamma^(tau-i) * u*a
    c_u = 0.0
    c_o = 0.0
    for tau in range(i, H):
        a = float(future_A_true[tau, n])
        u = float(future_U_true[tau, n])
        fac = (gamma ** (tau - i)) if use_discounted else 1.0
        c_u += fac * (u * a)
        c_o -= fac * (1.0 - a)
    c_r = robustness  # w_r * robustness added once at tau==i
    c_s = -1.0 if switch_happens else 0.0
    if switch_happens:
        run_len = _contiguous_available_run_len(future_A_true, i, n)
        horizon_left = max(1, H - i)
        hold_ratio = float(run_len) / float(horizon_left)
        jitter_pen = max(0.0, (jitter_thr - hold_ratio) / max(1e-6, jitter_thr))
        c_j = -jitter_pen
        pingpong = prev_node != -1 and current_node != -1 and prev_node == n and current_node != n
        c_p = -1.0 if pingpong else 0.0
    else:
        c_j = 0.0
        c_p = 0.0
    return (c_u, c_o, c_s, c_r, c_j, c_p)


def _reward_to_supervision_weight(reward_val: float, cfg_te: TrainEvalRealtimeConfig) -> float:
    """
    ë³´ìƒê°’ì„ ì•ˆì •ì ì¸ supervised RL-loss ê°€ì¤‘ì¹˜ë¡œ ë³€í™˜.
    """
    w = 1.0 + cfg_te.reward_guided_target_scale * math.tanh(reward_val / 2.0)
    w = float(np.clip(w, cfg_te.reward_guided_target_min_weight, cfg_te.reward_guided_target_max_weight))
    return w


# =========================================================
# Training (predicted history -> actual future target)
# =========================================================

def train_on_seed_pool(
    env_pairs: List[Dict[str, Any]],
    cfg_te: TrainEvalRealtimeConfig,
):
    device = get_device()
    print(f"ğŸš€ training start | device={device}")

    # num_nodes ê³ ì •
    first = env_pairs[0]
    _, N = first["actual_env"].A_final.shape

    L, H = cfg_te.L, cfg_te.H
    transformer = OnlineTransformerPredictor(num_nodes=N, L=L, H=H).to(device)
    online_consistency = OnlineConsistencyGenerator(num_nodes=N, H=H).to(device)

    target_consistency = copy.deepcopy(online_consistency).to(device)
    target_consistency.eval()
    for p in target_consistency.parameters():
        p.requires_grad = False

    reward_weights_module = None
    if getattr(cfg_te, "reward_learnable", False):
        reward_weights_module = LearnableRewardWeights(cfg_te, device)
        optimizer = optim.Adam(
            list(transformer.parameters())
            + list(online_consistency.parameters())
            + list(reward_weights_module.parameters()),
            lr=cfg_te.lr,
        )
        print("[train] reward weights are learnable (optimized with policy)")
    else:
        optimizer = optim.Adam(
            list(transformer.parameters()) + list(online_consistency.parameters()),
            lr=cfg_te.lr,
        )
    bce_loss_fn = nn.BCELoss()

    history_rows = []
    t0 = time.time()

    for epoch in range(cfg_te.epochs):
        transformer.train()
        online_consistency.train()

        epoch_loss = 0.0
        epoch_loss_tf = 0.0
        epoch_loss_rl = 0.0
        epoch_loss_cs = 0.0
        epoch_reward = 0.0
        step_count = 0

        for pair in env_pairs:
            actual_env = pair["actual_env"]
            pred_env = pair["pred_env"]

            A_in = np.asarray(pred_env.A_final, dtype=np.float32)        # ì…ë ¥ = predicted
            A_target = np.asarray(actual_env.A_final, dtype=np.float32)  # íƒ€ê¹ƒ = actual
            U_target = np.asarray(actual_env.utility, dtype=np.float32)  # ë³´ìƒ = actual utility

            T, N2 = A_in.shape
            if N2 != N:
                raise ValueError(f"num_nodes mismatch in training: expected {N}, got {N2}")

            # ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
            for t in range(L, T - H, cfg_te.stride):
                history_A = A_in[t - L : t, :]
                future_A_true = A_target[t : t + H, :]

                state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)
                future_tensor_true = torch.tensor(future_A_true, dtype=torch.float32, device=device).unsqueeze(0)

                optimizer.zero_grad()

                # 1) Transformer: predicted history -> actual future coverage
                future_pred = transformer(state_tensor)
                loss_tf = bce_loss_fn(future_pred, future_tensor_true)

                # 2) Consistency action generation (continuous target: [time_ratio, node_ratio])
                condition = future_pred.view(1, -1).detach()
                y_noisy_t2 = torch.randn(1, 2, device=device)
                y_noisy_t1 = y_noisy_t2 * 0.5

                step_2 = torch.tensor([[2.0]], device=device)
                step_1 = torch.tensor([[1.0]], device=device)

                y_curr = online_consistency(y_noisy_t1, condition, step_1)

                # 3) Reward-guided target (mini integrated reward, single-aircraft)
                if t - 1 >= 0:
                    current_node_approx = _best_available_node(A_target[t - 1], U_target[t - 1])
                else:
                    current_node_approx = -1
                if t - 2 >= 0:
                    prev_node_approx = _best_available_node(A_target[t - 2], U_target[t - 2])
                else:
                    prev_node_approx = -1

                gamma = float(cfg_te.reward_gamma)
                jitter_thr = float(getattr(cfg_te, "reward_jitter_min_hold_ratio", 0.35))
                use_discounted = bool(getattr(cfg_te, "reward_use_discounted_return", True))
                invalid_penalty = float(cfg_te.reward_invalid_penalty)

                time_offset_penalty = float(getattr(cfg_te, "reward_time_offset_penalty", 0.0))
                tie_breaker_eps = float(getattr(cfg_te, "reward_tie_breaker_epsilon", 1e-6))

                if reward_weights_module is not None:
                    w_u, w_o, w_s, w_r, w_j, w_p = reward_weights_module.to_numpy_weights()
                    best_i, best_n, reward_val = _find_best_reward_guided_target_with_weights(
                        future_A_true, U_target[t : t + H, :],
                        current_node_approx, prev_node_approx,
                        w_u, w_o, w_s, w_r, w_j, w_p,
                        gamma, jitter_thr, use_discounted, invalid_penalty,
                        time_offset_penalty=time_offset_penalty,
                        tie_breaker_epsilon=tie_breaker_eps,
                    )
                    coefs = _reward_coefficients_for_action(
                        future_A_true, U_target[t : t + H, :],
                        best_i, best_n, current_node_approx, prev_node_approx,
                        gamma, jitter_thr, use_discounted,
                    )
                    coef_tensor = torch.tensor(coefs, dtype=torch.float32, device=device)
                    reward_torch = (coef_tensor * reward_weights_module()).sum() - (time_offset_penalty * best_i)
                    rl_weight_torch = 1.0 + cfg_te.reward_guided_target_scale * torch.tanh(reward_torch / 2.0)
                    rl_weight_torch = torch.clamp(
                        rl_weight_torch,
                        cfg_te.reward_guided_target_min_weight,
                        cfg_te.reward_guided_target_max_weight,
                    )
                    reward_val = float(reward_torch.detach().item())
                else:
                    best_i, best_n, reward_val = _find_best_reward_guided_target(
                        future_A_true=future_A_true,
                        future_U_true=U_target[t : t + H, :],
                        current_node=current_node_approx,
                        cfg_te=cfg_te,
                        prev_node=prev_node_approx,
                    )
                    rl_weight_torch = _reward_to_supervision_weight(reward_val, cfg_te)

                epoch_reward += reward_val

                y_star = torch.tensor(
                    [[
                        0.0 if H <= 1 else (best_i / float(H - 1)),
                        0.0 if N <= 1 else (best_n / float(N - 1)),
                    ]],
                    dtype=torch.float32,
                    device=device,
                )
                loss_rl = rl_weight_torch * F.mse_loss(y_curr, y_star)

                # 4) Consistency self-consistency loss (EMA target)
                with torch.no_grad():
                    y_target = target_consistency(y_noisy_t2, condition, step_2)
                loss_cs = F.mse_loss(y_curr, y_target)

                total_loss = loss_tf + loss_rl + (cfg_te.lambda_consistency * loss_cs)
                total_loss.backward()
                optimizer.step()
                update_ema_target(online_consistency, target_consistency)

                epoch_loss += float(total_loss.item())
                epoch_loss_tf += float(loss_tf.item())
                epoch_loss_rl += float(loss_rl.item())
                epoch_loss_cs += float(loss_cs.item())
                step_count += 1

        row = {
            "epoch": epoch + 1,
            "steps": step_count,
            "avg_total_loss": epoch_loss / max(1, step_count),
            "avg_tf_loss": epoch_loss_tf / max(1, step_count),
            "avg_rl_loss": epoch_loss_rl / max(1, step_count),
            "avg_cs_loss": epoch_loss_cs / max(1, step_count),
            "avg_reward": epoch_reward / max(1, step_count),
            "elapsed_sec": time.time() - t0,
        }
        history_rows.append(row)
        print(
            f"[train] epoch {epoch+1:02d}/{cfg_te.epochs} | "
            f"loss={row['avg_total_loss']:.4f} (tf={row['avg_tf_loss']:.4f}, rl={row['avg_rl_loss']:.4f}, cs={row['avg_cs_loss']:.4f}) | "
            f"reward={row['avg_reward']:.4f} | elapsed={row['elapsed_sec']:.1f}s"
        )

    transformer.eval()
    online_consistency.eval()

    if reward_weights_module is not None:
        w = reward_weights_module.to_numpy_weights()
        print(f"[train] learned reward weights: w_utility={w[0]:.4f} w_outage={w[1]:.4f} w_switch={w[2]:.4f} w_robust={w[3]:.4f} w_jitter={w[4]:.4f} w_pingpong={w[5]:.4f}")

    save_weights(transformer, online_consistency, cfg_te.weight_path, reward_weights_module)

    hist_df = pd.DataFrame(history_rows)
    hist_df.to_csv(Path(cfg_te.results_dir) / "train_history_real_env.csv", index=False)
    print(f"âœ… train history saved: {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")

    return transformer, online_consistency, hist_df


# =========================================================
# Inference helpers (learned planner)
# =========================================================

def _predict_target_from_history(
    transformer,
    consistency,
    history_A: np.ndarray,
    H: int,
    N: int,
    device: torch.device,
    consistency_steps: int = 2,
) -> Tuple[int, int]:
    """
    history_A: (L, N) binary matrix
    return: (target_time_offset, target_node_idx)
    """
    state_tensor = torch.tensor(history_A, dtype=torch.float32, device=device).unsqueeze(0)

    with torch.no_grad():
        future_pred = transformer(state_tensor)
        condition = future_pred.view(1, -1)

        y_curr = torch.randn(1, 2, device=device)

        # configurable few-step consistency
        steps = max(1, int(consistency_steps))
        for s in range(steps, 0, -1):
            step_tensor = torch.tensor([[float(s)]], dtype=torch.float32, device=device)
            y_curr = consistency(y_curr, condition, step_tensor)

        target_time_offset = int(y_curr[0, 0].item() * (H - 1))
        target_node_idx = int(y_curr[0, 1].item() * (N - 1))

        target_time_offset = min(max(target_time_offset, 0), H - 1)
        target_node_idx = min(max(target_node_idx, 0), N - 1)

    return target_time_offset, target_node_idx


def build_learned_offline_plan_from_pred_env(
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    predicted envë§Œ ë³´ê³  ë§Œë“  learned plan (offline planning)
    """
    device = next(transformer.parameters()).device
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)
    T, N_env = A_pred.shape
    N_model = getattr(transformer, "num_nodes", N_env)
    if N_env > N_model:
        raise ValueError(
            f"Env has {N_env} nodes but checkpoint model expects {N_model}. "
            "Use weights trained with at least as many nodes as your env (e.g. larger max_sats)."
        )

    L, H = cfg_te.L, cfg_te.H
    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1

    for t in range(T):
        if t < L:
            available = np.where(A_pred[t] == 1)[0]
            if len(available) > 0:
                current_node = int(available[np.argmax(U_pred[t, available])])
            else:
                current_node = -1
            planned_idx[t] = current_node
            continue

        history_A = A_pred[t - L : t, :]  # (L, N_env)
        if N_env < N_model:
            history_A = np.pad(history_A, ((0, 0), (0, N_model - N_env)), mode="constant", constant_values=0)
        target_time_offset, target_node_idx = _predict_target_from_history(
            transformer=transformer,
            consistency=consistency,
            history_A=history_A,
            H=H,
            N=N_model,
            device=device,
            consistency_steps=consistency_steps,
        )
        target_node_idx = min(max(target_node_idx, 0), N_env - 1)

        # Bì•ˆ: target_time_offset <= rt_switch_time_offset_max ì´ë©´ "ì§€ê¸ˆ/ê³§ ìŠ¤ìœ„ì¹˜"ë¡œ í•´ì„
        k = int(getattr(cfg_te, "rt_switch_time_offset_max", 1))
        if (target_time_offset <= k) and A_pred[t, target_node_idx] == 1:
            current_node = int(target_node_idx)

        # predicted viewì—ì„œ ì´ë¯¸ ëŠê¸´ ê²½ìš°
        if current_node != -1 and A_pred[t, current_node] == 0:
            current_node = -1

        planned_idx[t] = current_node

    return planned_idx


def build_learned_realtime_corrected_plan(
    actual_env,
    pred_env,
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
    consistency_steps: int,
) -> np.ndarray:
    """
    ì‹¤ì‹œê°„ ë³´ì • ë²„ì „ (receding-horizon, MPC ìŠ¤íƒ€ì¼):
    - ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ predicted historyë¥¼ ì‚¬ìš©
    - ì§€ë‚˜ê°„ ìŠ¬ë¡¯ì€ ì‹¤ì œ ê´€ì¸¡(actual)ë¡œ ë®ì–´ì”€ (observed prefix correction)
    - ìŠ¬ë¡¯ë§ˆë‹¤ node reliability EMA ê°±ì‹ 
    - ì €ì‹ ë¢°/ë¹„ê°€ìš© ì œì•ˆì€ í˜„ì¬ ì‹¤ì œ ê°€ìš© í›„ë³´ë¡œ fallback
    - (ì˜µì…˜) HO ì•ˆì •í™” ê°€ë“œë ˆì¼: dwell + hysteresis + ping-pong ì–µì œ
      -> ê³¼ë„í•œ HO/í•‘íìœ¼ë¡œ latency/HO attemptsê°€ í­ì¦í•˜ëŠ” í˜„ìƒì„ ì™„í™”
    """
    device = next(transformer.parameters()).device

    A_actual = np.asarray(actual_env.A_final)
    U_actual = np.asarray(actual_env.utility)
    A_pred = np.asarray(pred_env.A_final)
    U_pred = np.asarray(pred_env.utility)

    T, N = A_actual.shape
    N_model = getattr(transformer, "num_nodes", N)
    if N > N_model:
        raise ValueError(
            f"Env has {N} nodes but checkpoint model expects {N_model}. "
            "Use weights trained with at least as many nodes as your env (e.g. larger max_sats)."
        )
    L, H = cfg_te.L, cfg_te.H

    planned_idx = np.full(T, -1, dtype=int)
    current_node = -1

    # ê³¼ê±° ê´€ì¸¡ ëˆ„ì  ë°˜ì˜ìš© (ì²˜ìŒì—” predictedë¡œ ì‹œì‘, ì§€ë‚˜ê°„ ìŠ¬ë¡¯ì€ actualë¡œ ë®ì–´ì”€)
    A_obs_corrected = A_pred.copy()
    # node ì‹ ë¢°ë„ (0~1)
    reliability = np.ones(N, dtype=np.float32)

    # ë§ˆì§€ë§‰ ìŠ¤ìœ„ì¹˜ ì‹œê°(ìŠ¬ë¡¯ ì¸ë±ìŠ¤)
    last_switch_t = -10**9

    # ë””ë²„ê·¸ ì¹´ìš´í„° (online TC ë™ì‘ ë¶„ì„ìš©)
    dbg_total_slots = T
    dbg_model_proposed = 0              # ëª¨ë¸ì´ target_time_offset==0ìœ¼ë¡œ switch ì œì•ˆí•œ ìŠ¬ë¡¯ ìˆ˜
    dbg_model_proposed_valid = 0        # ê·¸ ì¤‘ ì‹¤ì œ ê°€ìš© ìŠ¬ë¡¯ì¸ ê²½ìš°
    dbg_model_applied = 0               # ìµœì¢… í”Œëœì—ì„œ ëª¨ë¸ ì œì•ˆì´ ê·¸ëŒ€ë¡œ ë°˜ì˜ëœ ìŠ¬ë¡¯ ìˆ˜
    dbg_fallback_used = 0              # actual-slot greedy fallbackì´ ì‚¬ìš©ëœ ìŠ¬ë¡¯ ìˆ˜
    dbg_guardrail_block = 0            # guardrail ë•Œë¬¸ì— switchê°€ ë§‰íŒ íšŸìˆ˜

    def _score(t_: int, n_: int) -> float:
        # utility * reliability (ë‘˜ ë‹¤ 0~ëŒ€ëµ 1 scale ê¸°ëŒ€)
        return float(U_actual[t_, n_] * reliability[n_])

    for t in range(T):
        # í˜„ì¬ ìŠ¬ë¡¯ê¹Œì§€ëŠ” ì‹¤ì œ ê´€ì¸¡ê°’ ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •(ì‹¤ì‹œê°„ ì¸¡ì •)
        A_obs_corrected[t] = A_actual[t]

        prev1 = int(planned_idx[t - 1]) if t - 1 >= 0 else -1
        prev2 = int(planned_idx[t - 2]) if t - 2 >= 0 else -1

        # í˜„ì¬ ì—°ê²°ì˜ ìœ íš¨ì„±(í˜„ì¬ ìŠ¬ë¡¯ì—ì„œ ì»¤ë²„ë¦¬ì§€/ì •ì±…ìƒ ê°€ëŠ¥?)
        current_valid_now = (current_node >= 0 and current_node < N and A_actual[t, current_node] == 1)

        # ------------------------ (A) ê¸°ë³¸ ì œì•ˆ ì‚°ì¶œ ------------------------
        if t < L:
            # warmup: í˜„ì¬ ì‹¤ì œ ê°€ìš© í›„ë³´ ì¤‘ ì‹ ë¢°ë„ ë°˜ì˜ ì ìˆ˜ ìµœëŒ€ ì„ íƒ
            avail = np.where(A_actual[t] == 1)[0]
            if len(avail) > 0:
                score = U_actual[t, avail] * reliability[avail]
                proposed = int(avail[np.argmax(score)])
            else:
                proposed = -1

            corrected = proposed

        else:
            # historyëŠ” "ê³¼ê±° actual ê´€ì¸¡ ë°˜ì˜ + (ë¯¸ë˜ëŠ” ì—¬ì „íˆ predicted)"
            history_A = A_obs_corrected[t - L : t, :]  # (L, N)
            if N < N_model:
                history_A = np.pad(history_A, ((0, 0), (0, N_model - N)), mode="constant", constant_values=0)
            target_time_offset, target_node_idx = _predict_target_from_history(
                transformer=transformer,
                consistency=consistency,
                history_A=history_A,
                H=H,
                N=N_model,
                device=device,
                consistency_steps=consistency_steps,
            )
            target_node_idx = min(max(target_node_idx, 0), N - 1)

            k = int(getattr(cfg_te, "rt_switch_time_offset_max", 1))
            proposed = current_node
            if target_time_offset <= k:
                dbg_model_proposed += 1
                proposed = int(target_node_idx)

            # ì‹¤ì‹œê°„ ë³´ì • ë¡œì§
            proposed_valid_now = (
                proposed is not None
                and proposed >= 0
                and proposed < N
                and A_actual[t, proposed] == 1
            )

            if proposed_valid_now:
                dbg_model_proposed_valid += 1

            low_rel = False
            if proposed is not None and proposed >= 0 and proposed < N:
                low_rel = (float(reliability[proposed]) < cfg_te.rt_low_reliability_threshold)

            corrected = current_node

            # (ë³€ê²½) ëª¨ë¸ì´ í˜„ì¬ ìŠ¬ë¡¯ì—ì„œ ìœ íš¨í•œ ë…¸ë“œë¥¼ ì œì•ˆí•˜ë©´ ì‹ ë¢°ë„ì™€ ë¬´ê´€í•˜ê²Œ ì¼ë‹¨ ìˆ˜ìš©
            # (guardrailì—ì„œ ì¶”ê°€ í•„í„°ë§)
            model_based_switch = False
            if proposed_valid_now:
                corrected = proposed
                model_based_switch = True
            else:
                # proposedê°€ ì§€ê¸ˆ ì•ˆë˜ê±°ë‚˜ (ê°€ìš© X) í•˜ë©´ fallback
                if cfg_te.rt_use_actual_current_slot_fallback:
                    avail = np.where(A_actual[t] == 1)[0]
                    if len(avail) > 0:
                        # ì‹ ë¢°ë„ ë°˜ì˜ + í˜„ì¬ actual utility ê¸°ë°˜ ì¦‰ì‹œ ë³´ì •
                        score = U_actual[t, avail] * reliability[avail]
                        fallback = int(avail[np.argmax(score)])

                        # í˜„ì¬ ì—°ê²° ìœ ì§€ê°€ ìœ íš¨í•˜ê³  ì œì•ˆì´ low_relì´ë©´ ìœ ì§€ë¥¼ ìš°ì„ í•  ìˆ˜ë„ ìˆìŒ
                        if current_valid_now and low_rel:
                            corrected = current_node
                        else:
                            corrected = fallback
                            if corrected != current_node:
                                dbg_fallback_used += 1
                    else:
                        corrected = -1 if not current_valid_now else current_node
                else:
                    corrected = current_node if current_valid_now else -1

        # ------------------------ (B) HO ì•ˆì •í™” ê°€ë“œë ˆì¼ ------------------------
        if cfg_te.rt_enable_guardrails:
            # ìµœì¢… í›„ë³´ ìœ íš¨ì„±
            corrected_valid_now = (corrected is not None and corrected >= 0 and corrected < N and A_actual[t, corrected] == 1)

            # ìŠ¤ìœ„ì¹˜ ì‹œë„ì¸ì§€?
            switching = (corrected_valid_now and current_valid_now and corrected != current_node)

            if switching:
                blocked = False
                # (1) Dwell: ìµœê·¼ ìŠ¤ìœ„ì¹˜ í›„ ì¼ì • ì‹œê°„ì€ ìœ ì§€
                if (t - last_switch_t) < int(cfg_te.rt_min_dwell):
                    corrected = current_node
                    blocked = True
                else:
                    # (2) Hysteresis: ì¶©ë¶„í•œ ì´ë“ì´ ìˆì„ ë•Œë§Œ ìŠ¤ìœ„ì¹˜
                    s_cur = _score(t, current_node)
                    s_new = _score(t, corrected)

                    required_ratio = float(cfg_te.rt_hysteresis_ratio)

                    # (3) Ping-pong(A->B->A) ì–µì œ: t-2 ë…¸ë“œë¡œ ë˜ëŒì•„ê°€ë ¤ í•˜ë©´ ë” í° íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ìš”êµ¬
                    is_pingpong = (
                        corrected == prev2
                        and prev2 != -1
                        and prev1 == current_node
                        and (t - last_switch_t) <= int(cfg_te.rt_pingpong_window)
                    )
                    if is_pingpong:
                        required_ratio += float(cfg_te.rt_pingpong_extra_hysteresis_ratio)

                    ratio_ok = (s_new >= s_cur * (1.0 + required_ratio))
                    if float(cfg_te.rt_hysteresis_abs) > 0.0:
                        abs_ok = ((s_new - s_cur) >= float(cfg_te.rt_hysteresis_abs))
                        ok = ratio_ok and abs_ok
                    else:
                        ok = ratio_ok

                    if not ok:
                        corrected = current_node
                        blocked = True

                if blocked:
                    dbg_guardrail_block += 1

            # í˜„ì¬ ì—°ê²°ì´ ìœ íš¨í•œë° correctedê°€ -1ì´ë©´ ëŠê¹€ ë°©ì§€ë¡œ ìœ ì§€(ë‹¨, ì‹¤ì œë¡œ ìœ íš¨í•  ë•Œë§Œ)
            if corrected == -1 and current_valid_now:
                corrected = current_node

        # ------------------------ (C) ìµœì¢… ì ìš© & ìŠ¤ìœ„ì¹˜ ê¸°ë¡ ------------------------
        # ìµœì¢… ìœ íš¨ì„± ì¬ê²€ì‚¬
        if corrected != -1 and (corrected < 0 or corrected >= N or A_actual[t, corrected] != 1):
            corrected = -1

        # ìŠ¤ìœ„ì¹˜ ë°œìƒ ì‹œê° ê¸°ë¡ (ì—°ê²°ì´ ìœ íš¨í•œ ìƒíƒœì—ì„œ ë‹¤ë¥¸ ìœ íš¨ ë…¸ë“œë¡œ ë°”ë€ ê²½ìš°)
        if corrected != current_node and corrected != -1:
            last_switch_t = t

        # ë””ë²„ê·¸: ìµœì¢…ì ìœ¼ë¡œ ëª¨ë¸ ì œì•ˆì´ ì‹¤ì œë¡œ ë°˜ì˜ëœ ìŠ¬ë¡¯ ì¹´ìš´íŠ¸
        if t >= L and model_based_switch and corrected == proposed and corrected != current_node:
            dbg_model_applied += 1

        current_node = int(corrected) if corrected is not None else -1
        planned_idx[t] = current_node

        # ---- reliability update (í˜„ì¬ ìŠ¬ë¡¯ ê´€ì¸¡ìœ¼ë¡œ ë‹¤ìŒ ìŠ¬ë¡¯ planning ë³´ì •ì— ë°˜ì˜) ----
        match = (A_pred[t].astype(np.int32) == A_actual[t].astype(np.int32)).astype(np.float32)
        reliability = cfg_te.rt_reliability_alpha * reliability + (1.0 - cfg_te.rt_reliability_alpha) * match

        # false positive ì˜ˆì¸¡(ì˜ˆì¸¡=1, ì‹¤ì œ=0)ì—ëŠ” ì¶”ê°€ í˜ë„í‹°
        fp_mask = (A_pred[t] == 1) & (A_actual[t] == 0)
        reliability[fp_mask] *= cfg_te.rt_fp_extra_penalty

        reliability = np.clip(reliability, 0.05, 1.0)

    # ë””ë²„ê·¸ ë¡œê·¸ ì¶œë ¥ (ì˜µì…˜)
    if getattr(cfg_te, "rt_debug_log", False):
        total_effective = max(1, dbg_total_slots - cfg_te.L)
        print("[RT-TC debug]")
        print(f"  total_slots                = {dbg_total_slots}")
        print(f"  model_proposed_slots       = {dbg_model_proposed}")
        print(f"  model_proposed_valid_slots = {dbg_model_proposed_valid}")
        print(f"  model_applied_switches     = {dbg_model_applied}")
        print(f"  fallback_used_switches     = {dbg_fallback_used}")
        print(f"  guardrail_blocked_switches = {dbg_guardrail_block}")
        print(f"  model_applied_ratio        = {dbg_model_applied / float(total_effective):.4f}")

    return planned_idx


# =========================================================
# Evaluation on hold-out seeds
# =========================================================

def evaluate_on_test_seeds(
    test_pairs: List[Dict[str, Any]],
    transformer,
    consistency,
    cfg_te: TrainEvalRealtimeConfig,
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    rows = []
    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    print("\n=== Evaluation: Learned planner on held-out seeds ===")
    for pair in test_pairs:
        seed = pair["seed"]
        actual_env = pair["actual_env"]
        pred_env = pair["pred_env"]

        np.random.seed(seed)  # runtime miss stochasticity ì¬í˜„ì„± í™•ë³´

        # 1) Baseline: sustainable DAG on actual env
        dag_plan = sustainable_dag_plan(actual_env, cfg_te.exp_cfg)
        tl_dag, m_dag = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=dag_plan,
            method_name="Sustainable_DAG_NetworkX",
            mode="sustainable_dag",
            sampling_steps=0,
            cfg=cfg_te.exp_cfg,
        )
        m_dag["seed"] = seed
        rows.append(m_dag)

        # 2) Learned offline (predicted env only)
        offline_plan = build_learned_offline_plan_from_pred_env(
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_off, m_off = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=offline_plan,
            method_name="Learned_TC_Offline",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_off["seed"] = seed
        rows.append(m_off)

        # 3) Learned + realtime correction
        rt_plan = build_learned_realtime_corrected_plan(
            actual_env=actual_env,
            pred_env=pred_env,
            transformer=transformer,
            consistency=consistency,
            cfg_te=cfg_te,
            consistency_steps=cfg_te.consistency_steps_eval,
        )
        tl_rt, m_rt = execute_plan_on_env(
            build_res=actual_env,
            planned_idx=rt_plan,
            method_name="Learned_TC_RTCorrected",
            mode="consistency",
            sampling_steps=cfg_te.consistency_steps_eval,
            cfg=cfg_te.exp_cfg,
        )
        m_rt["seed"] = seed
        rows.append(m_rt)

        print(
            f"[seed={seed}]\n"
            f" ğŸ”¹ [DAG] Avail: {m_dag['Availability']:.4f} | QoE: {m_dag['EffectiveQoE']:.4f} | í•‘í: {m_dag['PingPong_Ratio']:.4f} | HOì‹¤íŒ¨: {m_dag['HO_Failure_Ratio']:.4f}\n"
            f" ğŸ”¸ [ AI] Avail: {m_rt['Availability']:.4f} | QoE: {m_rt['EffectiveQoE']:.4f} | í•‘í: {m_rt['PingPong_Ratio']:.4f} | HOì‹¤íŒ¨: {m_rt['HO_Failure_Ratio']:.4f}\n"
            f" --------------------------------------------------------"
        )

        # seed0 timeline ì €ì¥ (test ì²« seed)
        if seed == test_pairs[0]["seed"]:
            tl_dag.to_csv(results_dir / f"timeline_sustainable_dag_seed{seed}.csv", index=False)
            tl_off.to_csv(results_dir / f"timeline_learned_tc_offline_seed{seed}.csv", index=False)
            tl_rt.to_csv(results_dir / f"timeline_learned_tc_rtcorrected_seed{seed}.csv", index=False)

    df_runs = pd.DataFrame(rows)
    df_summary = summarize_runs(df_runs)

    df_runs.to_csv(results_dir / "exp3_learned_realtime_runs.csv", index=False)
    df_summary.to_csv(results_dir / "exp3_learned_realtime_summary.csv", index=False)

    print(f"âœ… saved: {results_dir / 'exp3_learned_realtime_runs.csv'}")
    print(f"âœ… saved: {results_dir / 'exp3_learned_realtime_summary.csv'}")
    return df_runs, df_summary


# =========================================================
# Main pipeline
# =========================================================

def main():
    cfg_te = TrainEvalRealtimeConfig()

    results_dir = Path(cfg_te.results_dir)
    results_dir.mkdir(parents=True, exist_ok=True)

    # experiments_atg_leoì˜ env builderê°€ ì“°ëŠ” TLE íŒŒì¼ í™•ì¸
    if not Path(cfg_te.exp_cfg.tle_path).exists():
        raise FileNotFoundError(
            f"Frozen TLE file not found: {cfg_te.exp_cfg.tle_path}\n"
            "ë¨¼ì € TLE íŒŒì¼ ê²½ë¡œë¥¼ ë§ì¶°ì£¼ì„¸ìš”."
        )

    # split ì €ì¥
    split_manifest = {
        "train_seeds": cfg_te.train_seeds,
        "val_seeds": cfg_te.val_seeds,
        "test_seeds": cfg_te.test_seeds,
    }
    with open(cfg_te.split_manifest_path, "w", encoding="utf-8") as f:
        json.dump(split_manifest, f, ensure_ascii=False, indent=2)

    print("=== Seed Split ===")
    print(json.dumps(split_manifest, ensure_ascii=False, indent=2))

    t0 = time.time()

    # 1) collect
    train_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.train_seeds)
    val_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.val_seeds)   # ì§€ê¸ˆì€ ë¡œê¹…ìš©/í™•ì¥ìš©
    test_pairs = collect_env_pairs(cfg_te.exp_cfg, cfg_te.test_seeds)

    print(f"[collect done] train={len(train_pairs)}, val={len(val_pairs)}, test={len(test_pairs)}")

    # 2) train
    transformer, consistency, train_hist = train_on_seed_pool(train_pairs, cfg_te)

    # (ì„ íƒ) val quick sanity metricsë¥¼ ì›í•˜ë©´ ì—¬ê¸° ì¶”ê°€ ê°€ëŠ¥
    # ì§€ê¸ˆì€ ë°”ë¡œ test í‰ê°€

    # 3) evaluate (hold-out)
    df_runs, df_summary = evaluate_on_test_seeds(
        test_pairs=test_pairs,
        transformer=transformer,
        consistency=consistency,
        cfg_te=cfg_te,
    )

    elapsed = time.time() - t0
    print("\n=== Done ===")
    print(f"Total elapsed: {elapsed:.2f} sec")
    print("Outputs:")
    print(f" - {cfg_te.weight_path}")
    print(f" - {Path(cfg_te.results_dir) / 'train_history_real_env.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_runs.csv'}")
    print(f" - {Path(cfg_te.results_dir) / 'exp3_learned_realtime_summary.csv'}")
    print(f" - {cfg_te.split_manifest_path}")

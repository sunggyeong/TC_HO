import torch
import torch.optim as optim
import numpy as np

# ì•ì„œ ë§Œë“  3ê°œì˜ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°
from tn_ntn_env_integrated import TN_NTN_Env
from transformer_predictor import TrajectoryPredictor
from consistency_handover import ConsistencyGenerator, HandoverRewardEvaluator

def train_agent():
    print("ğŸš€ 6G RAN AI ë””ì§€í„¸ íŠ¸ìœˆ - ëª¨ë¸ í•™ìŠµ(Training)ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # 1. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    EPOCHS = 10           # ì´ ë¹„í–‰(í•™ìŠµ) íšŸìˆ˜
    FLIGHT_TIME = 35      # 1íšŒ ë¹„í–‰ë‹¹ ì‹œë®¬ë ˆì´ì…˜ ì‹œê°„ (ì´ˆ)
    LEARNING_RATE = 1e-3  # í•™ìŠµë¥  (ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ë³´í­)
    
    # 2. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”
    transformer = TrajectoryPredictor(feature_dim=6, L=20, H=30)
    generator = ConsistencyGenerator(condition_dim=180)
    evaluator = HandoverRewardEvaluator(H=30)
    
    # 3. ì˜µí‹°ë§ˆì´ì €(Optimizer) ì„¤ì •
    # ì†Œí”„íŠ¸ì›¨ì–´ ìµœì í™”ì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” Adam ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
    # ë‘ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜)ë¥¼ ëª¨ë‘ ì—…ë°ì´íŠ¸ ëŒ€ìƒì— í¬í•¨
    optimizer = optim.Adam(
        list(transformer.parameters()) + list(generator.parameters()), 
        lr=LEARNING_RATE
    )
    
    # 4. ì—í”¼ì†Œë“œ(ë¹„í–‰) ë°˜ë³µ í•™ìŠµ ë£¨í”„
    for epoch in range(1, EPOCHS + 1):
        # ë§¤ ë¹„í–‰ë§ˆë‹¤ ìƒˆë¡œìš´ í™˜ê²½(ë””ì§€í„¸ íŠ¸ìœˆ) ì´ˆê¸°í™”
        env = TN_NTN_Env(seq_length=20)
        epoch_rewards = [] # ì´ë²ˆ ì—í”¼ì†Œë“œì—ì„œ íšë“í•œ ë³´ìƒ ê¸°ë¡
        
        print(f"=== [Epoch {epoch}/{EPOCHS}] ë¹„í–‰ ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ===")
        
        for t in range(1, FLIGHT_TIME + 1):
            raw_state, state_tensor = env.step()
            
            # ë²„í¼ê°€ ì°¨ì„œ(20ì´ˆ ì´í›„) AIê°€ ê°œì…í•  ìˆ˜ ìˆëŠ” ì‹œì 
            if state_tensor is not None:
                optimizer.zero_grad() # ì´ì „ ìŠ¤í…ì˜ ê¸°ìš¸ê¸°(Gradient) ì´ˆê¸°í™”
                
                # [ìˆœì „íŒŒ] 1. ë¯¸ë˜ 30ì´ˆ ì˜ˆì¸¡
                future_states = transformer(state_tensor)
                c_k = future_states.view(1, -1)
                
                # [ìˆœì „íŒŒ] 2. Consistency Modelë¡œ í•¸ë“œì˜¤ë²„ ì‹œì  ìƒì„± (ë…¸ì´ì¦ˆ -> ì •ë‹µ)
                y_curr = torch.randn(1, 1)
                steps = [torch.tensor([[2.0]]), torch.tensor([[1.0]])]
                for s in steps:
                    y_curr = generator(y_curr, c_k, s)
                    
                # [í‰ê°€] 3. ìƒì„±ëœ ì‹œì (y_curr)ì— ëŒ€í•œ ë³´ìƒ ê³„ì‚°
                reward, delta_t = evaluator.evaluate(y_curr, future_states)
                epoch_rewards.append(reward)
                
                # [ì—­ì „íŒŒ ë° í•™ìŠµ] 4. Loss ê³„ì‚° ë° ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸
                # ê°•í™”í•™ìŠµì˜ í•µì‹¬: Rewardë¥¼ ê·¹ëŒ€í™”í•´ì•¼ í•˜ë¯€ë¡œ LossëŠ” -Reward ë¡œ ì„¤ì •
                # (Gradient Ascent íš¨ê³¼ë¥¼ ë‚´ê¸° ìœ„í•œ PyTorch ëŒ€ë¦¬ ì†ì‹¤ í•¨ìˆ˜)
                loss = -torch.tensor(reward, requires_grad=True) * y_curr.mean() 
                
                loss.backward()   # ì˜¤ì°¨ ì—­ì „íŒŒ (Gradient ê³„ì‚°)
                optimizer.step()  # ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ (ë” ë˜‘ë˜‘í•´ì§!)
                
        # í•œ ë²ˆì˜ ë¹„í–‰(Epoch)ì´ ëë‚œ í›„ í‰ê·  ë³´ìƒ ì¶œë ¥
        avg_reward = np.mean(epoch_rewards)
        print(f"âœˆï¸ ë¹„í–‰ ì¢…ë£Œ | í‰ê·  Reward ì ìˆ˜: {avg_reward:.4f}")
        print("-" * 50)

    print("âœ… ëª¨ë“  í•™ìŠµì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! AIê°€ ìµœì ì˜ í•¸ë“œì˜¤ë²„ ì „ëµì„ í„°ë“í–ˆìŠµë‹ˆë‹¤.")
    
    # í•™ìŠµëœ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì €ì¥ (ì„ íƒ ì‚¬í•­)
    # torch.save(generator.state_dict(), 'trained_consistency_model.pth')
    # print("ğŸ’¾ í•™ìŠµëœ ëª¨ë¸ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    train_agent()